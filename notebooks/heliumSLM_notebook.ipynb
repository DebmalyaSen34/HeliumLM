{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13847301,"sourceType":"datasetVersion","datasetId":8820124},{"sourceId":13853747,"sourceType":"datasetVersion","datasetId":8819979},{"sourceId":660278,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":499385,"modelId":514625}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install datasets wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:36:28.232346Z","iopub.execute_input":"2025-11-25T12:36:28.233068Z","iopub.status.idle":"2025-11-25T12:36:37.210291Z","shell.execute_reply.started":"2025-11-25T12:36:28.233028Z","shell.execute_reply":"2025-11-25T12:36:37.209393Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.3.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.5.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (6.33.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.12.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.15.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pyarrow-22.0.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport torch.nn.functional as F\nimport os\nimport time\nimport json\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom torch.utils.data import IterableDataset, DataLoader\nimport copy\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-25T13:47:35.096295Z","iopub.execute_input":"2025-11-25T13:47:35.097222Z","iopub.status.idle":"2025-11-25T13:47:35.103817Z","shell.execute_reply.started":"2025-11-25T13:47:35.097187Z","shell.execute_reply":"2025-11-25T13:47:35.102383Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"## RoPE","metadata":{}},{"cell_type":"code","source":"def precompute_freq_cis(dim: int, end: int, theta: float = 10000.0):\n    \"\"\"Precomputes the angles for rotation.\n    Drawing a map of all possible positions ahead of time.\n\n    Args:\n        dim (int): The dimension of attention head.\n        end (int): The maximum sequence length.\n        theta (float, optional): The base frequency. Defaults to 10000.0 which is standard for Llama.\n    \"\"\"\n    \n    # Create a list of frequencies\n    # 1 / (theta ^ (2i / dim))\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    \n    # Create a list of positions: 0, 1, 2, ..., end-1\n    t = torch.arange(end, device=freqs.device)\n    \n    # Compute the outer product to get all position-frequency combinations\n    freqs = torch.outer(t, freqs).float() # Shape: [end, dim//2]\n    \n    # Turn them into polar coordinates (mag 1, angle freqs)\n    # Cis = cos + i*sin\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n    return freqs_cis\n\ndef reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    \"\"\"\n    Helper to match shapes.\n    freqs_cis: [max_len, dim // 2]\n    x: [Batch, seq_len, head, dim // 2]\n\n    Args:\n        freqs_cis (torch.Tensor): Polar coordinates of frequencies.\n        x (torch.Tensor): Input tensor.\n    \"\"\"\n    \n    ndim = x.ndim # Number of dimensions in x\n    assert 0 <= 1 < ndim # Ensure x has at least 2 dimensions\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1]) # Check shape compatibility\n    \n    # Reshape freqs_cis to [1, seq_len, 1, dim // 2] so that it broadcasts over Batch and Head\n    shape = [d if i==1 or i==ndim-1 else 1 for i, d in enumerate(x.shape)]\n    \n    return freqs_cis.view(*shape)\n\ndef apply_rotary_pos_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n    \"\"\"The Actual Rotation. We treat q/k vectors as complex numbers and multiply by the frequencies.\n\n    Args:\n        xq (torch.Tensor): x query tensor.\n        xk (torch.Tensor): x key tensor.\n        freqs_cis (torch.Tensor): Polar coordinates of frequencies.\n    \"\"\"\n    \n    # 1. Turn Query and Key into complex numbers\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n    \n    # 2. Get the specific frequencies for the current sequence length\n    freqs_cis = reshape_for_broadcast(freqs_cis=freqs_cis, x=xq_)\n    \n    # 3. Rotate (Multiply) and convert back to real numbers\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n    \n    return xq_out.type_as(xq), xk_out.type_as(xk)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:36:44.410497Z","iopub.execute_input":"2025-11-25T12:36:44.410942Z","iopub.status.idle":"2025-11-25T12:36:44.418676Z","shell.execute_reply.started":"2025-11-25T12:36:44.410923Z","shell.execute_reply":"2025-11-25T12:36:44.418047Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Attention","metadata":{}},{"cell_type":"code","source":"class EfficientAttention(nn.Module):\n    def __init__(self, d_model: int, n_head: int, n_kv_head: int, window_size: int):\n        super().__init__()\n        self.n_head = n_head # Number of query heads\n        self.n_kv_head = n_kv_head # Number of key/value heads\n        self.d_head = d_model // n_head # Dimension per head\n        self.window_size = window_size # Size of the local attention window\n        \n        #* The GQA Ratio (Grouped Query Attention Ratio)\n        #* If n_head=8 and n_kv_head=2, then n_rep=4\n        #* This means 1 K/V head will serve 4 query heads\n        self.n_rep = self.n_head // self.n_kv_head\n        \n        # Q needs full size: (d_model -> d_model)\n        #* Why? Because each query head is unique\n        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n        \n        #* Instead of mapping (d_model -> d_model), we map (d_model -> d_model / 4)\n        # Why? Because each K/V head is shared among multiple query heads\n        # This reduces the number of parameters and computation\n        self.k_proj = nn.Linear(d_model, self.n_kv_head * self.d_head, bias=False)\n        self.v_proj = nn.Linear(d_model, self.n_kv_head * self.d_head, bias=False)\n        \n        #* Output projection to combine heads back to d_model\n        self.output_proj = nn.Linear(d_model, d_model, bias=False)\n        \n    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n        B, T, C = x.size() # Batch size, sequence length, model dimension (Channels)\n        \n        # Calculate Q, K, V\n        # Q is standard shape: [Batch, Time, 8 heads, 32 dim]\n        q = self.q_proj(x).view(B, T, self.n_head, self.d_head).transpose(1, 2) #* Why transpose? -> [B, n_head, T, d_head] PyTorch's matrix multiplication (@) operates on the last two dimensions, so we need Time and Dim at the end.\n        \n        # K, V are reduced shape: [Batch, Time, 2 heads, 32 dim]        \n        # In standard attention, these would also be 8 heads\n        k = self.k_proj(x).view(B, T, self.n_kv_head, self.d_head).transpose(1, 2)\n        v = self.v_proj(x).view(B, T, self.n_kv_head, self.d_head).transpose(1, 2)\n        \n        #* Apply RoPE to Query and Key\n        # Before that we must transpose to [B, T, head, dim]\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        q, k = apply_rotary_pos_emb(q, k, freqs_cis=freqs_cis)\n        # Tranpose back to [B, head, T, dim]\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        \n        #* Since K and V have fewer heads, we need to repeat them\n        # so that they can align with the Q heads during attention computation\n        # We take the 2 KV heads and copy them 4 times each to get 8 \"virtual\" heads\n        # This allows the math to work without storing 8 unique heads in memory\n        k = k.repeat_interleave(self.n_rep, dim=1)\n        v = v.repeat_interleave(self.n_rep, dim=1)\n        \n        # Calculate Scores\n        #* It calculates how much every token relates to every other token\n        att = (q @ k.transpose(-2, -1)) * (1.0/math.sqrt(self.d_head))\n        \n        #* Sliding window Mask\n        \n        #* Casual Mask: \"Don't look into the future\"\n        # Creates a lower trianlge of 1s\n        casual_mask = torch.tril(torch.ones(T, T, device=x.device))\n        \n        #* Window Mask: \"Don't look too far back\"\n        # Creates an upper traingle starting from `window_size` back\n        # If window is 16, it blocks everything older than 16 steps ago\n        window_mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=-self.window_size+1)\n        \n        #* Combine masks\n        mask = casual_mask * window_mask # [T, T]\n        \n        # Apply Mask\n        att = att.masked_fill(mask == 0, float('-inf'))\n        \n        #* Softmax and Weighted Sum\n        att = F.softmax(att, dim=-1)\n        y = att @ v\n        \n        #* Reshape and Output Projection\n        # y: [B, n_head, T, d_head] -> [B, T, n_head, d_head] -> [B, T, C]\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        return self.output_proj(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:36:44.419401Z","iopub.execute_input":"2025-11-25T12:36:44.419668Z","iopub.status.idle":"2025-11-25T12:36:44.442041Z","shell.execute_reply.started":"2025-11-25T12:36:44.419651Z","shell.execute_reply":"2025-11-25T12:36:44.441513Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## RMSNorm Layer and Decoder Block","metadata":{}},{"cell_type":"code","source":"class RMSNorm(nn.Module):\n    \"\"\"Root Mean Square Layer Normalization.\n    Why: It is faster than LayerNorm because it skips calculatin the mean.\n    It only calculates variance.\n    \"\"\"\n    \n    def __init__(self, dim, eps = 1e-6):\n        super().__init__()\n\n        self.eps = eps # Prevents division by zero\n        \n        # The learnable weight parameter (gamma)\n        self.weight = nn.Parameter(torch.ones(dim))\n        \n    def _norm(self, x):\n        \n        # x.pow(2) -> squares each element\n        # .mean(-1, keepdim=True) -> mean over the last dimension (features)\n        # + self.eps -> add epsilon for numerical stability\n        # torch.rsqrt(...) -> reciprocal of the square root\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n    \n    def forward(self, x):\n        \n        # Convert to float32 for stability during normalization and later convert it back to original dtype\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n    \nclass SwiGLUMLP(nn.Module):\n    \"\"\"\n    Swiss Gated Linear Unit (SwiGLU) Feed-Forward Network.\n    Why: It learns efficiently and better than standard ReLU FFNs.\n    But it has 3 layers instead of 2.\n    \"\"\"\n    \n    def __init__(self, d_model, expansion_factor=2.5):\n        super().__init__()\n        \n        # Standard Transformer use expansion_factor of 4\n        # SLMs use ~2.6 (Llama) or lower to save memory\n        hidden_dim = int(d_model * expansion_factor)\n        \n        # 1. Gate projection - Determines which information to pass through\n        self.gate_proj = nn.Linear(d_model, hidden_dim, bias=False)\n        # 2. Up projection - Expands the dimensionality\n        self.up_proj = nn.Linear(d_model, hidden_dim, bias=False)\n        # 3. Down projection - Reduces back to d_model\n        self.down_proj = nn.Linear(hidden_dim, d_model, bias=False)\n        \n    def forward(self, x):\n        # Apply the SwiGLU activation: (SiLU(gate) * up_proj) -> down_proj\n        gate = F.silu(self.gate_proj(x))\n        up = self.up_proj(x)\n        \n        # Element-wise multiplication (gating mechanism)\n        fused = gate * up\n        \n        return self.down_proj(fused)\n    \nclass DecoderBlock(nn.Module):\n    \"\"\"A Single Transformer Block.\n    \n    Flow: Input -> RMSNorm -> Attention -> Residual -> RMSNorm -> FFN -> Residual\n    \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        \n        d_model = config['d_model']\n        n_head = config['n_head']\n        n_kv_head = config['n_kv_head']\n        window_size = config['window_size']\n        mlp_ratio = config.get('mlp_ratio', 2.5) # Default to 2.5 for efficiency\n        \n        # 1. Attention Engine\n        self.self_attn = EfficientAttention(\n            d_model=d_model,\n            n_head=n_head,\n            n_kv_head=n_kv_head,\n            window_size=window_size\n        )\n        \n        # 2, The Thinking Engine (FFN)\n        self.mlp = SwiGLUMLP(\n            d_model=d_model,\n            expansion_factor=mlp_ratio\n        )\n        \n        # 3. Normalization Layers (1 before attention, 1 before FFN)\n        self.input_layernorm = RMSNorm(d_model)\n        self.post_attn_layernorm = RMSNorm(d_model)\n        \n    def forward(self, x, freqs_cis):\n        \n        # 1. Attention Block with Residual Connection\n        # Norm before attention because it is more stable\n        residual = x\n        x = self.input_layernorm(x)\n        x = self.self_attn(x, freqs_cis)\n        x = residual + x\n        \n        # 2. MLP Block with Residual Connection\n        residual = x\n        x = self.post_attn_layernorm(x)\n        x = self.mlp(x)\n        x = residual + x\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:36:44.443451Z","iopub.execute_input":"2025-11-25T12:36:44.443922Z","iopub.status.idle":"2025-11-25T12:36:44.463375Z","shell.execute_reply.started":"2025-11-25T12:36:44.443905Z","shell.execute_reply":"2025-11-25T12:36:44.462851Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## TinySLM (Bringing together everything till now!)","metadata":{}},{"cell_type":"code","source":"class TinySLM(nn.Module):\n    \"\"\"A TinySLM model\"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        \n        self.config = config\n        \n        # 1. Embeddings\n        self.vocab_size = config['vocab_size']\n        self.d_model = config['d_model']\n        self.token_embedding = nn.Embedding(self.vocab_size, self.d_model)\n        \n        # 2. The Transformer Layers\n        # Implementating Block-Wise Weight Sharing\n        # If n_unique_layers < n_layers, we reuse the modules\n        self.layers = nn.ModuleList()\n        n_layers = config['n_layers']\n        \n        # Create the actual blocks\n        for _ in range(n_layers):\n            self.layers.append(DecoderBlock(config))\n            \n        # 3. Final Normalization\n        self.norm = RMSNorm(self.d_model)\n        \n        # 4. The Output Head\n        self.output = nn.Linear(self.d_model, self.vocab_size, bias=False)\n        \n        # 5. Weight Tying\n        # The matrix that turns Tokens -> Vectors is often the transpose of Vectors -> Tokens\n        # Sharing them saves memory and improves performance ~20-30%\n        self.token_embedding.weight = self.output.weight\n        \n        # 6. Precompute RoPE frequencies\n        # Compute enough for the max context window (eg. 2048)\n        self.freqs_cis = precompute_freq_cis(\n            dim=self.d_model // config['n_head'], # Dimension per head\n            end=config['max_seq_len'] * 2, # Just to be safe\n            theta=10000.0\n        )\n        \n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        # 1. Embed Tokens\n        x = self.token_embedding(idx) # Shape: [B, T, d_model]\n        \n        # 2. Prepare RoPE frequencies for current sequence length\n        freqs_cis = self.freqs_cis[:T].to(x.device)\n        \n        # 3. Run through Layers\n        for layer in self.layers:\n            x = layer(x, freqs_cis=freqs_cis)\n            \n        # 4. Final Normalization\n        x = self.norm(x)\n        \n        # 5. Calculate logits (if training)\n        logits = self.output(x)\n        \n        loss = None\n        if targets is not None:\n            # Flatten for cross-entropy [B*T, vocab_size]\n            loss = nn.functional.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                targets.view(-1)\n            )\n            \n        return logits, loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:36:44.463947Z","iopub.execute_input":"2025-11-25T12:36:44.464157Z","iopub.status.idle":"2025-11-25T12:36:44.484653Z","shell.execute_reply.started":"2025-11-25T12:36:44.464132Z","shell.execute_reply":"2025-11-25T12:36:44.483980Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"def train_tokenizer(input_file=\"/kaggle/input/textbook-dataset/hybrid_textbook_data.jsonl\", voacb_size=32000):\n    print(\"--------- Training Tokenizer ---------\")\n    \n    # 1. Initialize an empty BPE tokenizer\n    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n    tokenizer.pre_tokenizer = Whitespace()\n    \n    # 2. Setup Trainer\n    # Specialized tokens for controlling the model behavior\n    special_tokens = [\n        \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"\n    ]\n    trainer = BpeTrainer(\n        vocab_size=voacb_size,\n        special_tokens=special_tokens\n    )\n    \n    # 3. Extract text fields to temporary file\n    \n    temp_text_file = \"data/raw/temp_training_text.txt\"\n    os.makedirs(\"data/raw\", exist_ok=True)\n    \n    print(f\"Extracting text data from {input_file}...\")\n    line_count =0\n    \n    with open(input_file, 'r', encoding='utf-8') as infile, \\\n        open(temp_text_file, 'w', encoding='utf-8') as outfile:\n            \n            for line in infile:\n                try:\n                    data = json.loads(line.strip())\n                    text = data.get('text', '')\n                    \n                    if text and len(text.strip()) > 0:\n                        outfile.write(text + '\\n')\n                        line_count+=1\n                except json.JSONDecodeError:\n                    print(f\"Skipping invalid JSON line: {line.strip()}\")\n                    continue\n                \n    print(f\"Extracted {line_count} lines of text data to {temp_text_file}.\")\n    \n    # 4. Train tokenizer on extracted text\n    print(\"Training BPE tokenizer...\")\n    tokenizer.train([temp_text_file], trainer)\n    \n    # 5. Clean up temp file\n    os.remove(temp_text_file)\n    \n    # 6. Save the tokenizer\n    output_path = \"data/tokenizer/tiny_slm_tokenizer.json\"\n    os.makedirs(\"data/tokenizer\", exist_ok=True)\n    tokenizer.save(output_path)\n    print(f\"Tokenizer trained and saved at {output_path}\")\n    \n    return tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:36:44.913711Z","iopub.execute_input":"2025-11-25T12:36:44.914158Z","iopub.status.idle":"2025-11-25T12:36:44.920883Z","shell.execute_reply.started":"2025-11-25T12:36:44.914138Z","shell.execute_reply":"2025-11-25T12:36:44.920037Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_tokenizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:36:47.287456Z","iopub.execute_input":"2025-11-25T12:36:47.288148Z","iopub.status.idle":"2025-11-25T12:36:49.117186Z","shell.execute_reply.started":"2025-11-25T12:36:47.288119Z","shell.execute_reply":"2025-11-25T12:36:49.116497Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"--------- Training Tokenizer ---------\nExtracting text data from /kaggle/input/textbook-dataset/hybrid_textbook_data.jsonl...\nSkipping invalid JSON line: {\"topic\": \"Write a long and very detailed course unit for a textbook on \\\"Sustainable Transportation: Policy and Planning in Practice\\\" intended for young children.\\nWe are currently writing the first chapter: \\\"1. Introduction\\\".\\nWe have already covered the following sub-units in the current chapter: \\\"1.1. Overview of the course\\\".\\nWrite the new sub-unit titled \\\"1.2. Importance of sustainable transportation\\\" while trying to be:\\n- Rigorous - you create challenging textbooks that cover the material in depth.\\n- Engaging - your textbooks have a narrative arc and engaging tone, like the writing of Michael Lewis.\\n- Applied - you use specific and practical examples. For example, if the topic is integration in calculus, include equations and proofs of the concept you're teaching. As another example, if the topic is the history of the United States, include dates, names, and key events.\\nRemember this unit is intended for young children books, so use very simple, everyday words and phrases that a 10-year-old would easily understand. Tell an engaging and cheerful story and avoid any complex concepts or technical terms\", \"source\": \"cosmopedia_general\", \"style\": \"Write a Socratic dialogue between teacher and student.\", \"text\": \"## 1.2 Importance of Sustainable Transportation\\n### A Conversation with Emma\\n\\n[Scene: A sunny day, a small town square. Emma, a curious and bright 10-year-old, is sitting on a bench, looking at her teacher, Ms. Thompson, with wide eyes.]\\n\\nMs. Thompson: Hey there, Emma! Today we're going to talk about something super important: sustainable transportation.\\n\\nEmma: Oh, cool! What does that mean?\\n\\nMs. Thompson: Well, you know how we use cars and buses to get around every day? That's not always the best way for our planet.\\n\\nEmma: Why not? Cars are fun!\\n\\nMs. Thompson: I know they can be, but did you know that all those vehicles on the road make a lot of pollution? It's like when you leave your toys out and they get messy \\u2013 it's the same thing with the air we breathe!\\n\\nEmma: Ewww, gross! I don't want to breathe in yucky air.\\n\\nMs. Thompson: Exactly! And it's not just cars. Even airplanes and ships can make a big impact on our environment.\\n\\nEmma: But what about all those people who need to get around? How will they get to school or work?\\n\\nMs. Thompson: Ah, that's the best part! We can have sustainable transportation options that are good for us and the planet too!\\n\\n### The Problem with Traditional Transportation\\n\\n* **Air pollution**: Vehicles emit greenhouse gases like carbon dioxide and nitrogen oxides, which contribute to climate change.\\n* **Traffic congestion**: With more cars on the road, it takes longer to get where we need to go.\\n* **Health concerns**: Air pollution can cause respiratory problems and other health issues.\\n\\n### The Benefits of Sustainable Transportation\\n\\n* **Clean air**: Reducing emissions means cleaner air for everyone.\\n* **Less traffic**: Alternative modes like walking, biking, or taking public transportation reduce congestion.\\n* **Healthier communities**: With cleaner air and more physical activity, we'll see improved overall health.\\n\\n### Ways to Make a Difference\\n\\n#### Walk, Bike, or Skate!\\n\\n* Promote active transportation by creating safe routes for walkers, bikers, and skaters.\\n* Organize bike-to-school days or walk-a-thons with friends!\\n\\n#### Public Transportation is the Way!\\n\\n* Support public transportation systems like buses, trains, and subways.\\n* Encourage your parents to use public transport instead of driving.\\n\\n#### Electric Vehicles are the Future!\\n\\n* Learn about electric cars and how they reduce emissions.\\n* Share what you've learned with family and friends!\\n\\n#### Community Efforts Matter!\\n\\n* Join local groups or events focused on sustainability.\\n* Participate in clean-up initiatives or park beautification projects.\\n\\nEmma: Wow, I had no idea it was so important to make a change! What can I do right now?\\n\\nMs. Thompson: Well, Emma, you can start by talking to your parents about using public transport or carpooling when possible. You can also spread the word to your friends and family about the importance of sustainable transportation.\\n\\nEmma: That sounds like a plan! Thanks for teaching me something new, Ms. Thompson!\\n\\n[Scene closes with Emma walking away, feeling empowered to make a positive impact on her community.]\\n\\nThis chapter aims to introduce young children to the concept of sustainable transportation, highlighting its importance and the ways in which they can contribute to making a difference. By presenting real-world examples and engaging in a Socratic dialogue, we hope to spark curiosity and inspire action among our readers.\"}{\"topic\": \"Bioinformatics and Genomic Data Analysis (Mathematical foundations)\", \"source\": \"user_specialist\", \"style\": \"Create an infographic style summary.\", \"text\": \"# Bioinformatics & Genomic Data Analysis  \\n## Mathematical Foundations \\u2013 Infographic\\u2011Style Summary  \\n\\n---\\n\\n## 1\\ufe0f\\u20e3 What You\\u2019ll Learn  \\n| Section | Key Take\\u2011away | Why It Matters |\\n|--------|---------------|----------------|\\n| **Data Landscape** | DNA/RNA sequencing, microarrays, proteomics | Genomes are *data*; math turns them into *knowledge* |\\n| **Core Math Toolkit** | Linear algebra, probability, graph theory, information theory | Enables fast, accurate, and interpretable analyses |\\n| **Popular Algorithms** | Alignment, assembly, clustering, phylogeny | Solving real\\u2011world biological questions |\\n| **Hands\\u2011on Example** | Python snippets for sequence alignment & PCA | Practice what you read |\\n\\n---\\n\\n## 2\\ufe0f\\u20e3 Genomic Data: The Raw Material  \\n\\n- **Sequence Data**  \\n  - *Nucleotides*: A, C, G, T (plus ambiguous N)  \\n  - *Read length*: 50\\u2013300\\u202fbp (Illumina) to >\\u202f10\\u202fkb (PacBio/ONT)  \\n- **Quantitative Data**  \\n  - *Expression*: TPM, RPKM, raw counts  \\n  - *Variant calls*: genotype probabilities, allele frequencies  \\n- **High\\u2011Dimensionality**  \\n  - Thousands of genes \\u00d7 millions of samples \\u2192 curse of dimensionality  \\n\\n**Why**: Understanding data structure is the first step to choosing the right mathematical model.\\n\\n---\\n\\n## 3\\ufe0f\\u20e3 Core Mathematical Concepts  \\n\\n### 3.1 Linear Algebra  \\n- **Vectors & Matrices**: Represent gene expression profiles as vectors; similarity = dot product.  \\n- **Eigen\\u2011decomposition**: Principal Component Analysis (PCA) reduces dimensionality while preserving variance.  \\n  ```python\\n  # Example: PCA on RNA\\u2011seq counts\\n  from sklearn.decomposition import PCA\\n  pca = PCA(n_components=2)\\n  X_pca = pca.fit_transform(counts.T)   # genes \\u00d7 samples\\n  ```\\n- **Why**: Enables fast distance calculations, dimensionality reduction, and visualization.\\n\\n### 3.2 Probability & Statistics  \\n- **Random Variables**: Model sequencing errors, read counts (Poisson, Negative Binomial).  \\n- **Hypothesis Testing**: Differential expression (t\\u2011test, Wilcoxon).  \\n- **Bayesian Inference**: Variant calling (e.g., GATK HaplotypeCaller).  \\n- **Why**: Quantifies uncertainty, informs decision\\u2011making.\\n\\n### 3.3 Graph Theory  \\n- **De Bruijn Graphs**: Fundamental to *de\\u202fnovo* assembly; nodes = k\\u2011mers, edges = overlaps.  \\n- **Shortest Path**: Used in read alignment (e.g., BWA, Bowtie).  \\n- **Community Detection**: Gene\\u2011co\\u2011expression networks.  \\n- **Why**: Captures complex relationships efficiently.\\n\\n### 3.4 Information Theory  \\n- **Entropy**: Measures sequence complexity/variability.  \\n- **Mutual Information**: Detects regulatory relationships.  \\n- **Compression**: FASTQ files use gzip; alignment indices (BWT) compress genomes.  \\n- **Why**: Provides rigorous metrics for information content and similarity.\\n\\n---\\n\\n## 4\\ufe0f\\u20e3 Classic Algorithms & Their Math  \\n\\n| Problem | Typical Algorithm | Underlying Math | Key Parameters |\\n|---------|------------------|-----------------|----------------|\\n| **Sequence Alignment** | Needleman\\u2013Wunsch, Smith\\u2013Waterman | Dynamic programming (DP) on score matrix | Gap penalties, substitution matrix |\\n| **Read Mapping** | Burrows\\u2013Wheeler Transform + FM\\u2011index | String matching, suffix arrays | Seed length, allowed mismatches |\\n| **Assembly** | De\\u202fnovo (de Bruijn) | Graph traversal, Eulerian paths | k\\u2011mer size, coverage |\\n| **Phylogenetics** | Maximum Likelihood, Bayesian MCMC | Likelihood functions over trees | Model of evolution (JC69, GTR) |\\n| **Clustering** | k\\u2011means, hierarchical | Euclidean/Manhattan distances, linkage | Number of clusters, linkage type |\\n\\n**Why**: Each algorithm is a concrete application of the core math, showing how theory translates to practice.\\n\\n---\\n\\n## 5\\ufe0f\\u20e3 Hands\\u2011on: From Sequence to Insight  \\n\\n```python\\n# 1. Load FASTQ (reads)\\nfrom Bio import SeqIO\\nreads = [record.seq for record in SeqIO.parse(\\\"sample.fastq\\\", \\\"fastq\\\")]\\n\\n# 2. Simple k\\u2011mer count (k=5)\\nfrom collections import Counter\\nkmers = Counter(\\\"\\\".join([str(read[i:i+5]) for read in reads for i in range(len(read)-4)]))\\ntop_kmers = kmers.most_common(10)\\nprint(\\\"Top 10 5\\u2011mers:\\\", top_kmers)\\n\\n# 3. PCA on simulated expression data\\nimport numpy as np\\nnp.random.seed(42)\\nexpr = np.random.poisson(lam=10, size=(1000, 50))  # 1000 genes, 50 samples\\npca = PCA(n_components=2)\\npc_scores = pca.fit_transform(expr.T)\\nprint(\\\"Explained variance:\\\", pca.explained_variance_ratio_)\\n```\\n\\n**Why**: These snippets illustrate the bridge from raw data \\u2192 statistical modeling \\u2192 biological interpretation.\\n\\n---\\n\\n## 6\\ufe0f\\u20e3 Visualizing the Math  \\n\\n- **Heatmaps**: Gene\\u2011expression correlation matrices.  \\n- **Network Graphs**: Co\\u2011expression or protein\\u2011protein interaction networks.  \\n- **Phylogenetic Trees**: Visualize evolutionary relationships.  \\n- **Dimensionality Reduction Plots**: PCA, t\\u2011SNE, UMAP clusters.  \\n\\n> *Tip*: Use `seaborn`, `matplotlib`, and `networkx` for clean, publication\\u2011ready visuals.\\n\\n---\\n\\n## 7\\ufe0f\\u20e3 Quick Reference Cheat\\u2011Sheet  \\n\\n| Symbol | Meaning | Example |\\n|--------|---------|---------|\\n| **\\\\( \\\\mathbf{X} \\\\)** | Data matrix (genes \\u00d7 samples) | `counts` in the above code |\\n| **\\\\( \\\\mathbf{U}\\\\Sigma\\\\mathbf{V}^T \\\\)** | SVD decomposition | Used in PCA |\\n| **\\\\( H(X) \\\\)** | Shannon entropy | `-sum(p*log2(p))` |\\n| **\\\\( d_{ij} \\\\)** | Distance between sequences | Hamming distance |\\n| **\\\\( \\\\mathcal{L}(\\\\theta) \\\\)** | Likelihood | For phylogenetic tree |\\n\\n---\\n\\n## 8\\ufe0f\\u20e3 Take\\u2011home Questions  \\n\\n1. Why is a De Bruijn graph more suitable for short reads than an overlap graph?  \\n2. How does the choice of k\\u2011mer size affect assembly quality?  \\n3. In what scenarios would you prefer a Bayesian approach over a frequentist one in variant calling?  \\n\\n---\\n\\n### \\ud83c\\udf93 Final Thought  \\nMathematics is the *language* that turns raw genomic data into actionable biological insight. Mastering these foundations empowers you to design, evaluate, and innovate algorithms that drive discoveries in genomics.  \\n\\nHappy analyzing!\"}\nExtracted 1999 lines of text data to data/raw/temp_training_text.txt.\nTraining BPE tokenizer...\n\n\n\nTokenizer trained and saved at data/tokenizer/tiny_slm_tokenizer.json\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[{\"id\":0, \"content\":\"[PAD]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":1, \"content\":\"[UNK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":2, \"content\":\"[CLS]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":3, \"content\":\"[SEP]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}, {\"id\":4, \"content\":\"[MASK]\", \"single_word\":False, \"lstrip\":False, \"rstrip\":False, \"normalized\":False, \"special\":True}], normalizer=None, pre_tokenizer=Whitespace(), post_processor=None, decoder=None, model=BPE(dropout=None, unk_token=\"[UNK]\", continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"[PAD]\":0, \"[UNK]\":1, \"[CLS]\":2, \"[SEP]\":3, \"[MASK]\":4, \"!\":5, \"\"\":6, \"#\":7, \"$\":8, \"%\":9, \"&\":10, \"'\":11, \"(\":12, \")\":13, \"*\":14, \"+\":15, \",\":16, \"-\":17, \".\":18, \"/\":19, \"0\":20, \"1\":21, \"2\":22, \"3\":23, \"4\":24, \"5\":25, \"6\":26, \"7\":27, \"8\":28, \"9\":29, \":\":30, \";\":31, \"<\":32, \"=\":33, \">\":34, \"?\":35, \"@\":36, \"A\":37, \"B\":38, \"C\":39, \"D\":40, \"E\":41, \"F\":42, \"G\":43, \"H\":44, \"I\":45, \"J\":46, \"K\":47, \"L\":48, \"M\":49, \"N\":50, \"O\":51, \"P\":52, \"Q\":53, \"R\":54, \"S\":55, \"T\":56, \"U\":57, \"V\":58, \"W\":59, \"X\":60, \"Y\":61, \"Z\":62, \"[\":63, \"\\\":64, \"]\":65, \"^\":66, \"_\":67, \"`\":68, \"a\":69, \"b\":70, \"c\":71, \"d\":72, \"e\":73, \"f\":74, \"g\":75, \"h\":76, \"i\":77, \"j\":78, \"k\":79, \"l\":80, \"m\":81, \"n\":82, \"o\":83, \"p\":84, \"q\":85, \"r\":86, \"s\":87, \"t\":88, \"u\":89, \"v\":90, \"w\":91, \"x\":92, \"y\":93, \"z\":94, \"{\":95, \"|\":96, \"}\":97, \"~\":98, ...}, merges=[(\"i\", \"n\"), (\"t\", \"h\"), (\"a\", \"n\"), (\"e\", \"r\"), (\"o\", \"n\"), (\"e\", \"s\"), (\"e\", \"n\"), (\"t\", \"i\"), (\"o\", \"r\"), (\"a\", \"l\"), (\"in\", \"g\"), (\"r\", \"e\"), (\"a\", \"t\"), (\"th\", \"e\"), (\"an\", \"d\"), (\"i\", \"s\"), (\"t\", \"o\"), (\"e\", \"d\"), (\"r\", \"o\"), (\"ti\", \"on\"), (\"a\", \"r\"), (\"s\", \"t\"), (\"o\", \"f\"), (\"*\", \"*\"), (\"o\", \"u\"), (\"i\", \"c\"), (\"i\", \"t\"), (\"a\", \"c\"), (\"l\", \"e\"), (\"#\", \"#\"), (\"en\", \"t\"), (\"a\", \"s\"), (\"e\", \"l\"), (\"e\", \"c\"), (\"-\", \"-\"), (\"a\", \"tion\"), (\"o\", \"m\"), (\"f\", \"or\"), (\"T\", \"h\"), (\"e\", \"t\"), (\"o\", \"l\"), (\"v\", \"e\"), (\"p\", \"ro\"), (\"=\", \"=\"), (\"s\", \"i\"), (\"c\", \"h\"), (\"i\", \"l\"), (\"c\", \"e\"), (\"o\", \"w\"), (\"u\", \"n\"), (\"d\", \"e\"), (\"p\", \"l\"), (\"c\", \"on\"), (\"a\", \"m\"), (\"a\", \"b\"), (\"er\", \"s\"), (\"u\", \"r\"), (\"e\", \"m\"), (\"a\", \"p\"), (\"e\", \"x\"), (\"i\", \"g\"), (\"u\", \"s\"), (\"es\", \"s\"), (\"u\", \"l\"), (\"ou\", \"r\"), (\"c\", \"om\"), (\"Th\", \"e\"), (\"a\", \"d\"), (\"##\", \"#\"), (\"i\", \"m\"), (\"th\", \"at\"), (\"t\", \"r\"), (\"a\", \"g\"), (\"i\", \"r\"), (\"t\", \"er\"), (\"i\", \"th\"), (\"o\", \"p\"), (\"i\", \"d\"), (\"c\", \"an\"), (\"m\", \"ent\"), (\"a\", \"re\"), (\"--\", \"--\"), (\"it\", \"y\"), (\"s\", \"e\"), (\"a\", \"y\"), (\"at\", \"e\"), (\"ti\", \"c\"), (\"**\", \":\"), (\"t\", \"s\"), (\"s\", \"u\"), (\"w\", \"ith\"), (\"al\", \"l\"), (\"w\", \"h\"), (\"d\", \"i\"), (\"v\", \"er\"), (\"w\", \"e\"), (\"r\", \"es\"), (\"==\", \"==\"), (\"b\", \"e\"), ...]))"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class TextBookDataset(IterableDataset):\n    def __init__(self, file_path: str, tokenizer_path: str, max_length=512):\n        self.file_path = file_path\n        self.max_length = max_length\n        \n        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n        self.pad_token_id = self.tokenizer.token_to_id(\"[PAD]\")\n        \n    def __iter__(self):\n        \"\"\"\n        Generator that reads the file, tokenizes dynamically and yields chunks\n        \"\"\"\n        buffer_token = []\n        \n        with open(self.file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                \n                try:\n                    record = json.loads(line)\n                    text = record['text']\n                    \n                    # Tokenize\n                    encode = self.tokenizer.encode(text)\n                    ids = encode.ids\n                    \n                    # Add [EOS] token using [SEP] as a stand-in\n                    ids.append(self.tokenizer.token_to_id(\"[SEP]\"))\n                    \n                    buffer_token.extend(ids)\n                    \n                    # When buffer exceeds max_length, yield chunks\n                    while len(buffer_token) >= self.max_length:\n                        # Slice of a chunk\n                        chunk = buffer_token[:self.max_length]\n                        buffer_token = buffer_token[self.max_length:]\n                        \n                        # Prepare Input and Target\n                        # Input: [A, B, C, D]\n                        # Target: [B, C, D, E] (Next token prediction)\n                        \n                        # Ideally for training we just return the chunk\n                        # The training loop will handle shifting for next token prediction\n                        x = torch.tensor(chunk, dtype=torch.long)\n                        yield x, x # Target is same as input for next token prediction\n                except json.JSONDecodeError:\n                    continue","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:37:03.875633Z","iopub.execute_input":"2025-11-25T12:37:03.877046Z","iopub.status.idle":"2025-11-25T12:37:03.890655Z","shell.execute_reply.started":"2025-11-25T12:37:03.876988Z","shell.execute_reply":"2025-11-25T12:37:03.887783Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class ChatDataset(Dataset):\n    def __init__(self, tokenizer_path: str, max_seq_len: int=256):\n        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n        self.max_seq_len = max_seq_len\n        self.pad_token_id = self.tokenizer.token_to_id(\"[PAD]\")\n        \n        # Load dataset fom HuggingFace\n        print(\"Downloading dataset...\")\n        dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n        \n        self.samples = []\n        \n        for item in dataset:\n            \n            if item['context']:\n                prompt = f\"{item['instruction']}\\nContext: {item['context']}\"\n            else:\n                prompt = item['instruction']\n                \n            response = item['response']\n            \n            # Applying the chat format\n            # <|user|> ... <|assistant|> ...\n            formatted_text = (\n                f\"<|user|>\\n{prompt}\\n<|end|>\\n\"\n                f\"<|assistant|>\\n{response}\\n<|end|>\"\n            )\n            self.samples.append(formatted_text)\n            \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        text = self.samples[idx]\n        \n        # Tokenizer\n        ids = self.tokenizer.encode(text).ids\n        \n        # Pad or truncate\n        if len(ids) > self.max_seq_len:\n            ids = ids[:self.max_seq_len]\n        else:\n            # PAD with [PAD] token which is usually 0 or 1\n            pad_ids = self.tokenizer.token_to_id(\"[PAD]\")\n            ids = ids + [pad_ids]*(self.max_seq_len - len(ids))\n        \n        x = torch.tensor(ids, dtype=torch.long)\n        \n        # Target is same as input for casual language modeling\n        # In advance instruction tuning, the user prompt will be masked so that the model only learns to generate the response\n        return x, x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:37:04.772140Z","iopub.execute_input":"2025-11-25T12:37:04.772450Z","iopub.status.idle":"2025-11-25T12:37:04.779557Z","shell.execute_reply.started":"2025-11-25T12:37:04.772425Z","shell.execute_reply":"2025-11-25T12:37:04.778795Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import wandb\n\nwandb.login(key=\"84c972b22fec5ba717183e719f4b61a3cb688312\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:37:06.206566Z","iopub.execute_input":"2025-11-25T12:37:06.206864Z","iopub.status.idle":"2025-11-25T12:37:16.752896Z","shell.execute_reply.started":"2025-11-25T12:37:06.206844Z","shell.execute_reply":"2025-11-25T12:37:16.752203Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbatmancodes\u001b[0m (\u001b[33mbatmancodes-national-institute-of-technology-tiruchirappalli\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"CONFIG = {\n    # System\n    'device': 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu',\n    'num_workers': 0,      # Set to 0 for simpler debugging, 4 for speed\n    \n    # Model Architecture (Must match what we built)\n    'vocab_size': 32000,\n    'd_model': 256,\n    'n_layers': 8,\n    'n_head': 8,\n    'n_kv_head': 2,        # GQA\n    'window_size': 64,     # SWA\n    'max_seq_len': 256,    # Short context for faster training\n    'mlp_ratio': 2.5,\n    \n    # Training (The Optimizer Math)\n    'batch_size': 4,       # Micro-batch (fits in memory)\n    'accum_steps': 8,      # Virtual Batch Size = 4 * 8 = 32\n    'learning_rate': 3e-4, # Peak LR (standard for small models)\n    'max_epochs': 100,\n    'patience': 3,\n    'weight_decay': 0.01,  # AdamW regularization\n    'grad_clip': 1.0,      # Prevents exploding gradients\n    \n    # Data Paths\n    'train_file': '/kaggle/input/textbook-dataset/hybrid_textbook_data.jsonl',\n    'val_file': '/kaggle/input/textbook-dataset/validation_textbook.jsonl',\n    'tokenizer_path': '/kaggle/working/data/tokenizer/tiny_slm_tokenizer.json',\n    'save_dir': 'checkpoints',\n    \n    # Logging\n    'use_wandb': True,    # Set to True if you have an account\n    'log_interval': 10     # Print every 10 steps\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:38:14.300058Z","iopub.execute_input":"2025-11-25T12:38:14.300651Z","iopub.status.idle":"2025-11-25T12:38:14.348571Z","shell.execute_reply.started":"2025-11-25T12:38:14.300629Z","shell.execute_reply":"2025-11-25T12:38:14.347650Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def generate_sample(model, tokenizer_path, device, prompt=\"The cat\"):\n    from tokenizers import Tokenizer\n    \n    model.eval() # Switch to eval mode\n    tokenizer = Tokenizer.from_file(tokenizer_path)\n    \n    # Encode\n    ids = tokenizer.encode(prompt).ids\n    x = torch.tensor([ids], dtype=torch.long).to(device)\n    \n    # Generate 10 tokens\n    for _ in range(10):\n        with torch.no_grad():\n            logits, _ = model(x)\n            # Pick the last token's logits\n            next_token_logits = logits[0, -1, :] \n            # Greedy decode (pick max probability)\n            next_token = torch.argmax(next_token_logits).item()\n            \n            # Append\n            x = torch.cat((x, torch.tensor([[next_token]], device=device)), dim=1)\n    \n    # Decode\n    output_ids = x[0].tolist()\n    decoded = tokenizer.decode(output_ids)\n    print(f\"\\n[GENERATION SAMPLE]: {decoded}\\n\")\n    model.train() # Switch back to train mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:38:15.156386Z","iopub.execute_input":"2025-11-25T12:38:15.156641Z","iopub.status.idle":"2025-11-25T12:38:15.162463Z","shell.execute_reply.started":"2025-11-25T12:38:15.156623Z","shell.execute_reply":"2025-11-25T12:38:15.161795Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class EarlyStopping:\n    \"\"\"\n    The Watchdog. It counts how many did the validation failed to improve.\n    \"\"\"\n    \n    def __init__(self, patience: int =3, min_delta: float=0.0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter=0\n        self.best_loss = float('inf')\n        self.early_stop = False\n        \n    def __call__(self, val_loss: float) -> bool:\n        if val_loss < (self.best_loss - self.min_delta):\n            self.best_loss = val_loss\n            self.counter = 0 # Reset counter if we improved\n            return True # New Best model found\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n            return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:38:15.659923Z","iopub.execute_input":"2025-11-25T12:38:15.660209Z","iopub.status.idle":"2025-11-25T12:38:15.665418Z","shell.execute_reply.started":"2025-11-25T12:38:15.660189Z","shell.execute_reply":"2025-11-25T12:38:15.664520Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def evaluate(model: torch.nn.Module, val_loader: torch.utils.data.DataLoader, device: torch.device, vocab_size: int) -> float:\n    \"\"\"\n    Runs the model on the exam (validation set) without updating the weights\n\n    Args:\n        model (torch.nn.Module): The SLM model\n        val_loader (torch.utils.data.DataLoader): Validation data loader\n        device (torch.device): Device to run the model on (e.g., 'cuda', 'cpu')\n        vocab_size (int): Size of the vocabulary\n    \"\"\"\n    \n    model.eval()\n    total_loss = 0\n    steps=0\n    \n    with torch.no_grad():\n        for X, Y in val_loader:\n            X, Y = X.to(device), Y.to(device)\n            \n            logits, _ = model(X[:, :-1])\n            loss = nn.functional.cross_entropy(\n                logits.reshape(-1, vocab_size),\n                Y[:, 1:].reshape(-1)\n            )\n            total_loss += loss.item()\n            steps+=1\n    model.train()\n    return total_loss / steps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:38:16.092324Z","iopub.execute_input":"2025-11-25T12:38:16.093077Z","iopub.status.idle":"2025-11-25T12:38:16.098434Z","shell.execute_reply.started":"2025-11-25T12:38:16.093052Z","shell.execute_reply":"2025-11-25T12:38:16.097510Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def get_lr(it: int, max_iters: int, warmup_iters: int, min_lr: float, max_lr: float):\n    \"\"\"\n    Calculates the learning rate for the current iteration 'it'.\n    Implements Linear Warmup + Cosine Decay.\n\n    Args:\n        it (int): Current iteration number\n        max_iters (int): Total number of iterations\n        warmup_iters (int): Number of warmup iterations\n        min_lr (float): Minimum learning rate\n        max_lr (float): Maximum learning rate\n    \"\"\"\n    \n    # Linear Warmup\n    if it<warmup_iters:\n        return max_lr * (it+1) / warmup_iters\n    \n    # If we are past the end, return min_lr\n    if it>max_iters:\n        return min_lr\n    \n    # Cosine Decay\n    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n    coeff = 0.5 * (1+math.cos(math.pi*decay_ratio))\n    \n    return min_lr + coeff * (max_lr - min_lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:38:16.463243Z","iopub.execute_input":"2025-11-25T12:38:16.463808Z","iopub.status.idle":"2025-11-25T12:38:16.468605Z","shell.execute_reply.started":"2025-11-25T12:38:16.463784Z","shell.execute_reply":"2025-11-25T12:38:16.467696Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def train():\n    \n    # Setup\n    os.makedirs(CONFIG['save_dir'], exist_ok=True)\n    device = CONFIG['device']\n    \n    # Ensure Tokenizer Exists\n    # If the user hasn't trained a tokenizer yet, do it now\n    if not os.path.exists(CONFIG['tokenizer_path']):\n        print(\"Training tokenizer...\")\n        train_tokenizer(CONFIG['train_file'], CONFIG['vocab_size'])\n        \n    # Train Data Loader\n    train_ds = TextBookDataset(\n        file_path=CONFIG['train_file'],\n        tokenizer_path=CONFIG['tokenizer_path'],\n        max_length=CONFIG['max_seq_len']\n    )\n    \n    train_loader = DataLoader(\n        train_ds,\n        batch_size=CONFIG['batch_size'],\n        num_workers=CONFIG['num_workers'],\n        pin_memory=True if device == \"cuda\" else False\n    )\n    \n    if os.path.exists(CONFIG['val_file']):\n        # Validation Data Loader\n        val_ds = TextBookDataset(\n            file_path=CONFIG['val_file'],\n            tokenizer_path=CONFIG['tokenizer_path'],\n            max_length=CONFIG['max_seq_len']\n        )\n        \n        val_loader = DataLoader(\n            val_ds,\n            batch_size=CONFIG['batch_size'],\n            num_workers=CONFIG['num_workers'],\n            pin_memory=True if device == \"cuda\" else False\n        )\n    else:\n        print(\"Warning: Validation file not found. Skipping validation.\")\n        val_loader = None\n    \n    # Model Initialization\n    model = TinySLM(config=CONFIG).to(device)\n    optimzer = optim.AdamW(\n        model.parameters(),\n        lr=CONFIG['learning_rate'],\n        weight_decay=CONFIG['weight_decay'],\n        betas=(0.9, 0.95) # Standard for LLMs\n    )\n    \n    # Enable AMP (Automatic Mixed Precision) if using CUDA\n    scaler = torch.amp.GradScaler('cuda') if device == 'cuda' else None\n    \n    early_stopper = EarlyStopping(patience=CONFIG['patience'])\n    \n    # Logging\n    if CONFIG['use_wandb']:\n        wandb.init(project=\"tiny_slm_training\", config=CONFIG, name=\"trial-03\")\n        wandb.watch(model)\n        \n    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n    \n    # Loop variables\n    # We must estimate the total steps roughly since it's an IterableDataset\n    # Let's assume 1000 samples / 4 batch_size = 250 steps per epoch\n    est_steps_per_epoch = 1000 // CONFIG['batch_size']\n    max_iters = CONFIG['max_epochs'] * est_steps_per_epoch\n    warmup_iters = int(max_iters * 0.1) # 10% warmup\n    \n    iter_num=0\n    running_loss=0.0\n    \n    # We make a copy of the best model weights in RAM\n    best_model_weights = copy.deepcopy(model.state_dict())\n    \n    model.train()\n    \n    # Start Epochs\n    \n    for epoch in range(CONFIG['max_epochs']):\n        print(f\"Starting epoch {epoch+1}/{CONFIG['max_epochs']}...\")\n        t0 = time.time()\n        \n        for batch_idx, (X, Y) in enumerate(train_loader):\n            \n            # Update the learning rate: Cosine Scheduler\n            lr = get_lr(iter_num, max_iters, warmup_iters, 3e-5, CONFIG['learning_rate'])\n            for param_group in optimzer.param_groups:\n                param_group['lr'] = lr\n                \n            # Move data to device\n            X, Y = X.to(device), Y.to(device)\n            \n            # Create targets (Next Token Prediction)\n            # In input:'the cat sat', target:'cat sat on'\n            \n            input_ids = X[:, :-1]\n            targets = Y[:, 1:]\n            \n            # Forward Pass (with AMP if CUDA)\n            if scaler:\n                with torch.amp.autocast('cuda'):\n                    logits, _ = model(input_ids)\n                    \n                    loss = nn.functional.cross_entropy(\n                        logits.reshape(-1, CONFIG['vocab_size']),\n                        targets.reshape(-1)\n                    )\n            else:\n                # MPS\n                logits, _ = model(input_ids)\n                loss = nn.functional.cross_entropy(\n                    logits.reshape(-1, CONFIG['vocab_size']),\n                    targets.reshape(-1)\n                )\n                \n            # Gradient Accumulation Scaling\n            # If we want a virtual bach size of 32 but can only fit 4, we simple divide loss by 8. Summing 8 small gradients = 1 big gradient\n            loss = loss / CONFIG['accum_steps']\n            \n            # Backward Pass\n            if scaler:\n                scaler.scale(loss).backward()\n            else:\n                loss.backward()\n                \n            running_loss += loss.item() * CONFIG['accum_steps'] # Scale back for logging\n            \n            # Optimizer Step (after accum_steps)\n            if (batch_idx + 1) % CONFIG['accum_steps'] == 0:\n                # Capture gradient norm\n                if scaler:\n                    scaler.unscale_(optimzer)\n                    \n                # clip_grad_norm_ returns the norm before clipping\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n                \n                # Update Weights\n                if scaler:\n                    scaler.step(optimzer)\n                    scaler.update()\n                else:\n                    optimzer.step()\n                    \n                # Flush gradients\n                optimzer.zero_grad(set_to_none=True)\n                \n                iter_num +=1\n                \n                # Logging\n                if iter_num % CONFIG['log_interval'] == 0:\n                    \n                    # Compute Perplexity\n                    avg_loss = running_loss / (CONFIG['log_interval']*CONFIG['accum_steps'])\n                    perplexity = math.exp(avg_loss) if avg_loss < 20 else -1\n                    \n                    # Calculate Tokens per second (throughput)\n                    # We processed (batch_size * seq_len * accum_steps * log_interval) tokens\n                    tokens_processed = (CONFIG['batch_size'] * CONFIG['max_seq_len'] * CONFIG['accum_steps'] * CONFIG['log_interval'])\n                    \n                    dt = time.time() - t0\n                    tokens_per_sec = tokens_processed / dt\n                    t0 = time.time()\n                    \n                    # Calculate Memory (if CUDA)\n                    mem_usage = 0\n                    if device == 'cuda':\n                        mem_usage = torch.cuda.max_memory_allocated()/1024**2 # in MB\n                        torch.cuda.reset_peak_memory_stats() # reset for next logging\n                    \n                    print(f\"step {iter_num} | loss: {avg_loss:.4f} | ppl: {perplexity:.1f} | \"\n                          f\"norm: {grad_norm:.2f} | mem: {mem_usage:.0f}MB | {tokens_per_sec:.0f} tok/s\")\n                    \n                    if CONFIG['use_wandb']:\n                        wandb.log({\n                            \"train/loss\": avg_loss,\n                            \"train/perplexity\": perplexity,\n                            \"train/learning_rate\": lr,\n                            \"train/grad_norm\": grad_norm,\n                            \"perf/tokens_per_sec\": tokens_per_sec,\n                            \"perf/memory_MB\": mem_usage\n                        })\n                    running_loss = 0.0\n                    \n        # Validation\n        val_loss = 0.0\n        if val_loader is not None:\n            print(\"Running Validation...\", end=\"\")\n            val_loss = evaluate(model, val_loader, device, vocab_size=CONFIG['vocab_size'])\n            val_ppl = math.exp(val_loss) if val_loss <20 else -1\n            print(f\" Val Loss: {val_loss:.4f} | Val PPL: {val_ppl:.1f}\")\n            \n            # Early Stopping Check\n            is_new_best = early_stopper(val_loss)\n            \n            if is_new_best:\n                print(\"Found New Best Model! Saving checkpoint...\")\n                best_model_weights = copy.deepcopy(model.state_dict())\n                torch.save(best_model_weights, f\"{CONFIG['save_dir']}/best_model.pt\")\n                \n            if early_stopper.early_stop:\n                print(\"Early stopping triggered. Ending training.\")\n                print(\"Restoring best model weights...\")\n                model.load_state_dict(best_model_weights)\n                break\n                    \n        print(f\"Saving Checkpoint for Epoch {epoch+1}...\")\n        torch.save(model.state_dict(), f\"{CONFIG['save_dir']}/model_epoch_{epoch+1}.pt\")\n        \n    print(\"Training Complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:38:57.916595Z","iopub.execute_input":"2025-11-25T12:38:57.917229Z","iopub.status.idle":"2025-11-25T12:38:57.936632Z","shell.execute_reply.started":"2025-11-25T12:38:57.917203Z","shell.execute_reply":"2025-11-25T12:38:57.935730Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T12:38:58.565373Z","iopub.execute_input":"2025-11-25T12:38:58.566214Z","iopub.status.idle":"2025-11-25T13:00:53.375632Z","shell.execute_reply.started":"2025-11-25T12:38:58.566181Z","shell.execute_reply":"2025-11-25T13:00:53.374739Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251125_123858-ji2vw0s0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/batmancodes-national-institute-of-technology-tiruchirappalli/tiny_slm_training/runs/ji2vw0s0' target=\"_blank\">trial-03</a></strong> to <a href='https://wandb.ai/batmancodes-national-institute-of-technology-tiruchirappalli/tiny_slm_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/batmancodes-national-institute-of-technology-tiruchirappalli/tiny_slm_training' target=\"_blank\">https://wandb.ai/batmancodes-national-institute-of-technology-tiruchirappalli/tiny_slm_training</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/batmancodes-national-institute-of-technology-tiruchirappalli/tiny_slm_training/runs/ji2vw0s0' target=\"_blank\">https://wandb.ai/batmancodes-national-institute-of-technology-tiruchirappalli/tiny_slm_training/runs/ji2vw0s0</a>"},"metadata":{}},{"name":"stdout","text":"Model Parameters: 13.44M\nStarting epoch 1/100...\nstep 10 | loss: 10.5097 | ppl: 36668.1 | norm: 2.10 | mem: 935MB | 20064 tok/s\nstep 20 | loss: 10.4981 | ppl: 36248.0 | norm: 2.18 | mem: 935MB | 26114 tok/s\nstep 30 | loss: 10.4668 | ppl: 35129.4 | norm: 2.19 | mem: 935MB | 25665 tok/s\nstep 40 | loss: 10.4280 | ppl: 33791.9 | norm: 2.13 | mem: 935MB | 26156 tok/s\nstep 50 | loss: 10.3679 | ppl: 31822.9 | norm: 2.37 | mem: 935MB | 26111 tok/s\nstep 60 | loss: 10.2917 | ppl: 29487.7 | norm: 2.21 | mem: 935MB | 25989 tok/s\nstep 70 | loss: 10.1948 | ppl: 26763.2 | norm: 2.27 | mem: 935MB | 26442 tok/s\nstep 80 | loss: 10.0897 | ppl: 24094.4 | norm: 2.04 | mem: 935MB | 26266 tok/s\nstep 90 | loss: 9.9940 | ppl: 21895.3 | norm: 1.95 | mem: 935MB | 25885 tok/s\nstep 100 | loss: 9.8866 | ppl: 19665.1 | norm: 1.70 | mem: 935MB | 26145 tok/s\nstep 110 | loss: 9.7997 | ppl: 18028.2 | norm: 1.60 | mem: 935MB | 26271 tok/s\nstep 120 | loss: 9.7272 | ppl: 16767.6 | norm: 1.52 | mem: 935MB | 25859 tok/s\nstep 130 | loss: 9.6594 | ppl: 15667.6 | norm: 1.44 | mem: 935MB | 24422 tok/s\nstep 140 | loss: 9.5827 | ppl: 14511.2 | norm: 1.47 | mem: 935MB | 25989 tok/s\nstep 150 | loss: 9.5281 | ppl: 13740.9 | norm: 1.46 | mem: 935MB | 25798 tok/s\nstep 160 | loss: 9.5101 | ppl: 13495.1 | norm: 1.62 | mem: 935MB | 25937 tok/s\nstep 170 | loss: 9.5615 | ppl: 14207.7 | norm: 1.52 | mem: 935MB | 25850 tok/s\nRunning Validation... Val Loss: 9.4114 | Val PPL: 12226.7\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 1...\nStarting epoch 2/100...\nstep 180 | loss: 10.3198 | ppl: 30325.8 | norm: 1.52 | mem: 935MB | 129043 tok/s\nstep 190 | loss: 9.3651 | ppl: 11673.4 | norm: 1.38 | mem: 934MB | 26352 tok/s\nstep 200 | loss: 9.2639 | ppl: 10550.1 | norm: 1.42 | mem: 934MB | 26084 tok/s\nstep 210 | loss: 9.2077 | ppl: 9973.9 | norm: 1.36 | mem: 934MB | 25952 tok/s\nstep 220 | loss: 9.1367 | ppl: 9289.9 | norm: 1.36 | mem: 934MB | 26215 tok/s\nstep 230 | loss: 9.0767 | ppl: 8749.0 | norm: 1.24 | mem: 934MB | 26218 tok/s\nstep 240 | loss: 9.0070 | ppl: 8159.8 | norm: 1.32 | mem: 934MB | 25669 tok/s\nstep 250 | loss: 8.9180 | ppl: 7465.3 | norm: 1.27 | mem: 934MB | 25237 tok/s\nstep 260 | loss: 8.8505 | ppl: 6977.8 | norm: 1.25 | mem: 934MB | 26399 tok/s\nstep 270 | loss: 8.7629 | ppl: 6392.6 | norm: 1.31 | mem: 934MB | 26123 tok/s\nstep 280 | loss: 8.6789 | ppl: 5877.4 | norm: 1.26 | mem: 934MB | 25645 tok/s\nstep 290 | loss: 8.5966 | ppl: 5413.1 | norm: 1.25 | mem: 934MB | 25811 tok/s\nstep 300 | loss: 8.5441 | ppl: 5136.5 | norm: 1.25 | mem: 934MB | 25905 tok/s\nstep 310 | loss: 8.4639 | ppl: 4740.4 | norm: 1.24 | mem: 934MB | 25657 tok/s\nstep 320 | loss: 8.3587 | ppl: 4267.1 | norm: 1.20 | mem: 934MB | 26054 tok/s\nstep 330 | loss: 8.2619 | ppl: 3873.5 | norm: 1.18 | mem: 934MB | 26029 tok/s\nstep 340 | loss: 8.3210 | ppl: 4109.4 | norm: 1.18 | mem: 934MB | 25758 tok/s\nstep 350 | loss: 8.4892 | ppl: 4861.9 | norm: 1.19 | mem: 934MB | 25795 tok/s\nRunning Validation... Val Loss: 8.0594 | Val PPL: 3163.5\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 2...\nStarting epoch 3/100...\nstep 360 | loss: 9.0465 | ppl: 8489.0 | norm: 1.25 | mem: 935MB | 64138 tok/s\nstep 370 | loss: 7.9284 | ppl: 2774.9 | norm: 1.07 | mem: 935MB | 25781 tok/s\nstep 380 | loss: 7.8241 | ppl: 2500.1 | norm: 1.06 | mem: 935MB | 25193 tok/s\nstep 390 | loss: 7.7759 | ppl: 2382.6 | norm: 1.03 | mem: 935MB | 26247 tok/s\nstep 400 | loss: 7.6800 | ppl: 2164.6 | norm: 0.92 | mem: 935MB | 26043 tok/s\nstep 410 | loss: 7.6008 | ppl: 1999.9 | norm: 0.95 | mem: 935MB | 26031 tok/s\nstep 420 | loss: 7.5316 | ppl: 1866.1 | norm: 0.95 | mem: 935MB | 26220 tok/s\nstep 430 | loss: 7.4430 | ppl: 1707.8 | norm: 0.88 | mem: 935MB | 25981 tok/s\nstep 440 | loss: 7.3956 | ppl: 1628.8 | norm: 0.87 | mem: 935MB | 26220 tok/s\nstep 450 | loss: 7.2951 | ppl: 1473.0 | norm: 0.90 | mem: 935MB | 25984 tok/s\nstep 460 | loss: 7.2598 | ppl: 1422.0 | norm: 0.81 | mem: 935MB | 25637 tok/s\nstep 470 | loss: 7.1726 | ppl: 1303.3 | norm: 0.88 | mem: 935MB | 25955 tok/s\nstep 480 | loss: 7.1881 | ppl: 1323.6 | norm: 0.98 | mem: 935MB | 26196 tok/s\nstep 490 | loss: 7.1912 | ppl: 1327.7 | norm: 1.05 | mem: 935MB | 25751 tok/s\nstep 500 | loss: 7.0498 | ppl: 1152.7 | norm: 1.14 | mem: 935MB | 25477 tok/s\nstep 510 | loss: 7.0350 | ppl: 1135.7 | norm: 1.13 | mem: 935MB | 26290 tok/s\nstep 520 | loss: 7.4820 | ppl: 1775.7 | norm: 1.15 | mem: 935MB | 26210 tok/s\nstep 530 | loss: 7.6794 | ppl: 2163.4 | norm: 1.12 | mem: 935MB | 26052 tok/s\nRunning Validation... Val Loss: 7.0197 | Val PPL: 1118.5\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 3...\nStarting epoch 4/100...\nstep 540 | loss: 7.9740 | ppl: 2904.3 | norm: 1.65 | mem: 935MB | 43568 tok/s\nstep 550 | loss: 6.8690 | ppl: 962.0 | norm: 1.13 | mem: 934MB | 25985 tok/s\nstep 560 | loss: 6.8374 | ppl: 932.0 | norm: 0.91 | mem: 934MB | 25771 tok/s\nstep 570 | loss: 6.8182 | ppl: 914.3 | norm: 1.13 | mem: 934MB | 25848 tok/s\nstep 580 | loss: 6.7380 | ppl: 843.9 | norm: 0.86 | mem: 934MB | 25834 tok/s\nstep 590 | loss: 6.7492 | ppl: 853.4 | norm: 1.29 | mem: 934MB | 26552 tok/s\nstep 600 | loss: 6.7053 | ppl: 816.7 | norm: 1.27 | mem: 934MB | 26544 tok/s\nstep 610 | loss: 6.6403 | ppl: 765.3 | norm: 1.29 | mem: 934MB | 26754 tok/s\nstep 620 | loss: 6.6477 | ppl: 771.0 | norm: 1.13 | mem: 934MB | 26623 tok/s\nstep 630 | loss: 6.5561 | ppl: 703.5 | norm: 1.06 | mem: 934MB | 25968 tok/s\nstep 640 | loss: 6.5471 | ppl: 697.2 | norm: 0.88 | mem: 934MB | 26739 tok/s\nstep 650 | loss: 6.5423 | ppl: 693.9 | norm: 1.18 | mem: 934MB | 26489 tok/s\nstep 660 | loss: 6.5436 | ppl: 694.8 | norm: 1.32 | mem: 934MB | 26665 tok/s\nstep 670 | loss: 6.6001 | ppl: 735.2 | norm: 1.18 | mem: 934MB | 26677 tok/s\nstep 680 | loss: 6.4501 | ppl: 632.8 | norm: 1.60 | mem: 934MB | 26465 tok/s\nstep 690 | loss: 6.4633 | ppl: 641.1 | norm: 1.15 | mem: 934MB | 26575 tok/s\nstep 700 | loss: 7.2161 | ppl: 1361.1 | norm: 1.41 | mem: 934MB | 26695 tok/s\nstep 710 | loss: 7.2859 | ppl: 1459.5 | norm: 1.23 | mem: 934MB | 26379 tok/s\nRunning Validation... Val Loss: 6.5431 | Val PPL: 694.4\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 4...\nStarting epoch 5/100...\nstep 720 | loss: 7.2572 | ppl: 1418.2 | norm: 1.42 | mem: 935MB | 33186 tok/s\nstep 730 | loss: 6.2966 | ppl: 542.7 | norm: 1.58 | mem: 935MB | 27001 tok/s\nstep 740 | loss: 6.3152 | ppl: 552.9 | norm: 1.59 | mem: 935MB | 26589 tok/s\nstep 750 | loss: 6.2894 | ppl: 538.8 | norm: 1.30 | mem: 935MB | 25870 tok/s\nstep 760 | loss: 6.2330 | ppl: 509.3 | norm: 1.40 | mem: 935MB | 26658 tok/s\nstep 770 | loss: 6.2502 | ppl: 518.1 | norm: 2.00 | mem: 935MB | 26491 tok/s\nstep 780 | loss: 6.1907 | ppl: 488.2 | norm: 1.34 | mem: 935MB | 26694 tok/s\nstep 790 | loss: 6.1538 | ppl: 470.5 | norm: 1.14 | mem: 935MB | 26496 tok/s\nstep 800 | loss: 6.1900 | ppl: 487.9 | norm: 1.83 | mem: 935MB | 26792 tok/s\nstep 810 | loss: 6.0693 | ppl: 432.4 | norm: 1.57 | mem: 935MB | 26296 tok/s\nstep 820 | loss: 6.1007 | ppl: 446.2 | norm: 1.59 | mem: 935MB | 26712 tok/s\nstep 830 | loss: 6.1027 | ppl: 447.1 | norm: 1.14 | mem: 935MB | 26799 tok/s\nstep 840 | loss: 6.0866 | ppl: 439.9 | norm: 1.71 | mem: 935MB | 26365 tok/s\nstep 850 | loss: 6.1233 | ppl: 456.4 | norm: 2.58 | mem: 935MB | 26764 tok/s\nstep 860 | loss: 6.0298 | ppl: 415.6 | norm: 1.28 | mem: 935MB | 26813 tok/s\nstep 870 | loss: 6.0114 | ppl: 408.0 | norm: 1.66 | mem: 935MB | 26580 tok/s\nstep 880 | loss: 7.0566 | ppl: 1160.5 | norm: 1.31 | mem: 935MB | 26035 tok/s\nstep 890 | loss: 6.9454 | ppl: 1038.4 | norm: 1.15 | mem: 935MB | 26803 tok/s\nRunning Validation... Val Loss: 6.1665 | Val PPL: 476.5\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 5...\nStarting epoch 6/100...\nstep 900 | loss: 6.6014 | ppl: 736.1 | norm: 1.87 | mem: 935MB | 26846 tok/s\nstep 910 | loss: 5.8386 | ppl: 343.3 | norm: 1.36 | mem: 934MB | 26740 tok/s\nstep 920 | loss: 5.8970 | ppl: 364.0 | norm: 1.77 | mem: 934MB | 26929 tok/s\nstep 930 | loss: 5.8469 | ppl: 346.1 | norm: 1.61 | mem: 934MB | 26538 tok/s\nstep 940 | loss: 5.8027 | ppl: 331.2 | norm: 1.77 | mem: 934MB | 26700 tok/s\nstep 950 | loss: 5.8714 | ppl: 354.7 | norm: 1.32 | mem: 934MB | 26821 tok/s\nstep 960 | loss: 5.7811 | ppl: 324.1 | norm: 2.11 | mem: 934MB | 26714 tok/s\nstep 970 | loss: 5.7516 | ppl: 314.7 | norm: 2.12 | mem: 934MB | 26911 tok/s\nstep 980 | loss: 5.7828 | ppl: 324.7 | norm: 1.42 | mem: 934MB | 26908 tok/s\nstep 990 | loss: 5.6771 | ppl: 292.1 | norm: 1.57 | mem: 934MB | 26822 tok/s\nstep 1000 | loss: 5.6938 | ppl: 297.0 | norm: 1.57 | mem: 934MB | 25759 tok/s\nstep 1010 | loss: 5.7573 | ppl: 316.5 | norm: 2.31 | mem: 934MB | 26605 tok/s\nstep 1020 | loss: 5.7475 | ppl: 313.4 | norm: 2.00 | mem: 934MB | 26713 tok/s\nstep 1030 | loss: 5.7519 | ppl: 314.8 | norm: 1.64 | mem: 934MB | 26542 tok/s\nstep 1040 | loss: 5.6590 | ppl: 286.9 | norm: 1.74 | mem: 934MB | 26793 tok/s\nstep 1050 | loss: 5.8989 | ppl: 364.6 | norm: 3.29 | mem: 934MB | 26744 tok/s\nstep 1060 | loss: 6.7150 | ppl: 824.7 | norm: 1.49 | mem: 934MB | 26648 tok/s\nRunning Validation... Val Loss: 5.9017 | Val PPL: 365.7\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 6...\nStarting epoch 7/100...\nstep 1070 | loss: 7.0845 | ppl: 1193.4 | norm: 3.51 | mem: 935MB | 132177 tok/s\nstep 1080 | loss: 5.6060 | ppl: 272.1 | norm: 1.85 | mem: 935MB | 26786 tok/s\nstep 1090 | loss: 5.4854 | ppl: 241.1 | norm: 1.33 | mem: 935MB | 26607 tok/s\nstep 1100 | loss: 5.5287 | ppl: 251.8 | norm: 1.45 | mem: 935MB | 26900 tok/s\nstep 1110 | loss: 5.4951 | ppl: 243.5 | norm: 1.63 | mem: 935MB | 26718 tok/s\nstep 1120 | loss: 5.4804 | ppl: 239.9 | norm: 1.77 | mem: 935MB | 25831 tok/s\nstep 1130 | loss: 5.5267 | ppl: 251.3 | norm: 1.50 | mem: 935MB | 26802 tok/s\nstep 1140 | loss: 5.4099 | ppl: 223.6 | norm: 1.90 | mem: 935MB | 26764 tok/s\nstep 1150 | loss: 5.4876 | ppl: 241.7 | norm: 1.88 | mem: 935MB | 26722 tok/s\nstep 1160 | loss: 5.4169 | ppl: 225.2 | norm: 1.57 | mem: 935MB | 26712 tok/s\nstep 1170 | loss: 5.3588 | ppl: 212.5 | norm: 1.48 | mem: 935MB | 26687 tok/s\nstep 1180 | loss: 5.3577 | ppl: 212.2 | norm: 1.93 | mem: 935MB | 26735 tok/s\nstep 1190 | loss: 5.4777 | ppl: 239.3 | norm: 1.63 | mem: 935MB | 26497 tok/s\nstep 1200 | loss: 5.4781 | ppl: 239.4 | norm: 1.66 | mem: 935MB | 26873 tok/s\nstep 1210 | loss: 5.4115 | ppl: 224.0 | norm: 1.99 | mem: 935MB | 26882 tok/s\nstep 1220 | loss: 5.3618 | ppl: 213.1 | norm: 1.72 | mem: 935MB | 26620 tok/s\nstep 1230 | loss: 5.8556 | ppl: 349.2 | norm: 2.33 | mem: 935MB | 26716 tok/s\nstep 1240 | loss: 6.4341 | ppl: 622.7 | norm: 1.67 | mem: 935MB | 26735 tok/s\nRunning Validation... Val Loss: 5.6586 | Val PPL: 286.8\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 7...\nStarting epoch 8/100...\nstep 1250 | loss: 6.6241 | ppl: 753.1 | norm: 2.28 | mem: 935MB | 67130 tok/s\nstep 1260 | loss: 5.2834 | ppl: 197.0 | norm: 2.00 | mem: 934MB | 26925 tok/s\nstep 1270 | loss: 5.1723 | ppl: 176.3 | norm: 1.56 | mem: 934MB | 26873 tok/s\nstep 1280 | loss: 5.2450 | ppl: 189.6 | norm: 1.61 | mem: 934MB | 26608 tok/s\nstep 1290 | loss: 5.1654 | ppl: 175.1 | norm: 2.04 | mem: 934MB | 26708 tok/s\nstep 1300 | loss: 5.2106 | ppl: 183.2 | norm: 1.71 | mem: 934MB | 26358 tok/s\nstep 1310 | loss: 5.2126 | ppl: 183.6 | norm: 2.20 | mem: 934MB | 26539 tok/s\nstep 1320 | loss: 5.1716 | ppl: 176.2 | norm: 1.90 | mem: 934MB | 26806 tok/s\nstep 1330 | loss: 5.2187 | ppl: 184.7 | norm: 1.79 | mem: 934MB | 26906 tok/s\nstep 1340 | loss: 5.1076 | ppl: 165.3 | norm: 2.35 | mem: 934MB | 26720 tok/s\nstep 1350 | loss: 5.1135 | ppl: 166.3 | norm: 1.76 | mem: 934MB | 26462 tok/s\nstep 1360 | loss: 5.0629 | ppl: 158.0 | norm: 1.60 | mem: 934MB | 26812 tok/s\nstep 1370 | loss: 5.1892 | ppl: 179.3 | norm: 1.87 | mem: 934MB | 25856 tok/s\nstep 1380 | loss: 5.2709 | ppl: 194.6 | norm: 2.17 | mem: 934MB | 26520 tok/s\nstep 1390 | loss: 5.0920 | ppl: 162.7 | norm: 2.04 | mem: 934MB | 26716 tok/s\nstep 1400 | loss: 5.1196 | ppl: 167.3 | norm: 2.33 | mem: 934MB | 26925 tok/s\nstep 1410 | loss: 5.8852 | ppl: 359.7 | norm: 2.64 | mem: 934MB | 26542 tok/s\nstep 1420 | loss: 6.1798 | ppl: 482.9 | norm: 1.88 | mem: 934MB | 26672 tok/s\nRunning Validation... Val Loss: 5.4532 | Val PPL: 233.5\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 8...\nStarting epoch 9/100...\nstep 1430 | loss: 6.1798 | ppl: 482.9 | norm: 3.10 | mem: 935MB | 44739 tok/s\nstep 1440 | loss: 4.9717 | ppl: 144.3 | norm: 1.80 | mem: 935MB | 26630 tok/s\nstep 1450 | loss: 4.9368 | ppl: 139.3 | norm: 1.86 | mem: 935MB | 26655 tok/s\nstep 1460 | loss: 4.9587 | ppl: 142.4 | norm: 1.60 | mem: 935MB | 26753 tok/s\nstep 1470 | loss: 4.8868 | ppl: 132.5 | norm: 1.58 | mem: 935MB | 26497 tok/s\nstep 1480 | loss: 4.9813 | ppl: 145.7 | norm: 2.23 | mem: 935MB | 26774 tok/s\nstep 1490 | loss: 4.9716 | ppl: 144.3 | norm: 2.08 | mem: 935MB | 26792 tok/s\nstep 1500 | loss: 4.9255 | ppl: 137.8 | norm: 1.71 | mem: 935MB | 25679 tok/s\nstep 1510 | loss: 4.9766 | ppl: 145.0 | norm: 2.03 | mem: 935MB | 26760 tok/s\nstep 1520 | loss: 4.8292 | ppl: 125.1 | norm: 1.62 | mem: 935MB | 26824 tok/s\nstep 1530 | loss: 4.8719 | ppl: 130.6 | norm: 1.85 | mem: 935MB | 26720 tok/s\nstep 1540 | loss: 4.8966 | ppl: 133.8 | norm: 2.20 | mem: 935MB | 25810 tok/s\nstep 1550 | loss: 4.9092 | ppl: 135.5 | norm: 2.06 | mem: 935MB | 26537 tok/s\nstep 1560 | loss: 5.0470 | ppl: 155.6 | norm: 1.80 | mem: 935MB | 26207 tok/s\nstep 1570 | loss: 4.8679 | ppl: 130.1 | norm: 2.05 | mem: 935MB | 26155 tok/s\nstep 1580 | loss: 4.9128 | ppl: 136.0 | norm: 1.52 | mem: 935MB | 26518 tok/s\nstep 1590 | loss: 5.8961 | ppl: 363.6 | norm: 2.33 | mem: 935MB | 25061 tok/s\nstep 1600 | loss: 6.0160 | ppl: 409.9 | norm: 1.79 | mem: 935MB | 25238 tok/s\nRunning Validation... Val Loss: 5.2912 | Val PPL: 198.6\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 9...\nStarting epoch 10/100...\nstep 1610 | loss: 5.6912 | ppl: 296.2 | norm: 1.77 | mem: 935MB | 33196 tok/s\nstep 1620 | loss: 4.6978 | ppl: 109.7 | norm: 2.07 | mem: 934MB | 25712 tok/s\nstep 1630 | loss: 4.7249 | ppl: 112.7 | norm: 1.85 | mem: 934MB | 26019 tok/s\nstep 1640 | loss: 4.7198 | ppl: 112.1 | norm: 1.71 | mem: 934MB | 26285 tok/s\nstep 1650 | loss: 4.6784 | ppl: 107.6 | norm: 1.57 | mem: 934MB | 26349 tok/s\nstep 1660 | loss: 4.7761 | ppl: 118.6 | norm: 1.80 | mem: 934MB | 26164 tok/s\nstep 1670 | loss: 4.7135 | ppl: 111.4 | norm: 1.87 | mem: 934MB | 26324 tok/s\nstep 1680 | loss: 4.7175 | ppl: 111.9 | norm: 2.25 | mem: 934MB | 26152 tok/s\nstep 1690 | loss: 4.7625 | ppl: 117.0 | norm: 1.92 | mem: 934MB | 26403 tok/s\nstep 1700 | loss: 4.5907 | ppl: 98.6 | norm: 2.03 | mem: 934MB | 26412 tok/s\nstep 1710 | loss: 4.6913 | ppl: 109.0 | norm: 1.91 | mem: 934MB | 26597 tok/s\nstep 1720 | loss: 4.6876 | ppl: 108.6 | norm: 2.10 | mem: 934MB | 26537 tok/s\nstep 1730 | loss: 4.6956 | ppl: 109.5 | norm: 2.15 | mem: 934MB | 26620 tok/s\nstep 1740 | loss: 4.7987 | ppl: 121.4 | norm: 2.46 | mem: 934MB | 26331 tok/s\nstep 1750 | loss: 4.7209 | ppl: 112.3 | norm: 2.25 | mem: 934MB | 25839 tok/s\nstep 1760 | loss: 4.7106 | ppl: 111.1 | norm: 2.39 | mem: 934MB | 26072 tok/s\nstep 1770 | loss: 5.9273 | ppl: 375.1 | norm: 2.05 | mem: 934MB | 26281 tok/s\nstep 1780 | loss: 5.8596 | ppl: 350.6 | norm: 1.85 | mem: 934MB | 26347 tok/s\nRunning Validation... Val Loss: 5.1356 | Val PPL: 170.0\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 10...\nStarting epoch 11/100...\nstep 1790 | loss: 5.2162 | ppl: 184.2 | norm: 1.99 | mem: 935MB | 26286 tok/s\nstep 1800 | loss: 4.4276 | ppl: 83.7 | norm: 1.67 | mem: 935MB | 26247 tok/s\nstep 1810 | loss: 4.5370 | ppl: 93.4 | norm: 2.43 | mem: 935MB | 26146 tok/s\nstep 1820 | loss: 4.4833 | ppl: 88.5 | norm: 1.82 | mem: 935MB | 26261 tok/s\nstep 1830 | loss: 4.4691 | ppl: 87.3 | norm: 1.84 | mem: 935MB | 26553 tok/s\nstep 1840 | loss: 4.6006 | ppl: 99.5 | norm: 1.98 | mem: 935MB | 26548 tok/s\nstep 1850 | loss: 4.5045 | ppl: 90.4 | norm: 2.59 | mem: 935MB | 26403 tok/s\nstep 1860 | loss: 4.5356 | ppl: 93.3 | norm: 2.12 | mem: 935MB | 26435 tok/s\nstep 1870 | loss: 4.5374 | ppl: 93.4 | norm: 2.20 | mem: 935MB | 25481 tok/s\nstep 1880 | loss: 4.3856 | ppl: 80.3 | norm: 2.18 | mem: 935MB | 26402 tok/s\nstep 1890 | loss: 4.4583 | ppl: 86.3 | norm: 2.15 | mem: 935MB | 26517 tok/s\nstep 1900 | loss: 4.5301 | ppl: 92.8 | norm: 2.62 | mem: 935MB | 26789 tok/s\nstep 1910 | loss: 4.5617 | ppl: 95.7 | norm: 2.38 | mem: 935MB | 26477 tok/s\nstep 1920 | loss: 4.5894 | ppl: 98.4 | norm: 1.97 | mem: 935MB | 26642 tok/s\nstep 1930 | loss: 4.5344 | ppl: 93.2 | norm: 2.33 | mem: 935MB | 26766 tok/s\nstep 1940 | loss: 4.8065 | ppl: 122.3 | norm: 3.13 | mem: 935MB | 26653 tok/s\nstep 1950 | loss: 5.6947 | ppl: 297.3 | norm: 1.98 | mem: 935MB | 26440 tok/s\nRunning Validation... Val Loss: 5.0492 | Val PPL: 155.9\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 11...\nStarting epoch 12/100...\nstep 1960 | loss: 6.0446 | ppl: 421.8 | norm: 3.56 | mem: 935MB | 131692 tok/s\nstep 1970 | loss: 4.4161 | ppl: 82.8 | norm: 1.82 | mem: 934MB | 26551 tok/s\nstep 1980 | loss: 4.2523 | ppl: 70.3 | norm: 1.68 | mem: 934MB | 26743 tok/s\nstep 1990 | loss: 4.3246 | ppl: 75.5 | norm: 2.16 | mem: 934MB | 26810 tok/s\nstep 2000 | loss: 4.2921 | ppl: 73.1 | norm: 1.96 | mem: 934MB | 25857 tok/s\nstep 2010 | loss: 4.3130 | ppl: 74.7 | norm: 2.18 | mem: 934MB | 26448 tok/s\nstep 2020 | loss: 4.3978 | ppl: 81.3 | norm: 1.95 | mem: 934MB | 26475 tok/s\nstep 2030 | loss: 4.3041 | ppl: 74.0 | norm: 2.39 | mem: 934MB | 26497 tok/s\nstep 2040 | loss: 4.4076 | ppl: 82.1 | norm: 2.02 | mem: 934MB | 26241 tok/s\nstep 2050 | loss: 4.2947 | ppl: 73.3 | norm: 2.20 | mem: 934MB | 26705 tok/s\nstep 2060 | loss: 4.2265 | ppl: 68.5 | norm: 2.34 | mem: 934MB | 26693 tok/s\nstep 2070 | loss: 4.2505 | ppl: 70.1 | norm: 1.96 | mem: 934MB | 26099 tok/s\nstep 2080 | loss: 4.3796 | ppl: 79.8 | norm: 2.30 | mem: 934MB | 26713 tok/s\nstep 2090 | loss: 4.4272 | ppl: 83.7 | norm: 2.17 | mem: 934MB | 27021 tok/s\nstep 2100 | loss: 4.3764 | ppl: 79.6 | norm: 2.23 | mem: 934MB | 26620 tok/s\nstep 2110 | loss: 4.3451 | ppl: 77.1 | norm: 1.97 | mem: 934MB | 26758 tok/s\nstep 2120 | loss: 4.9144 | ppl: 136.2 | norm: 2.66 | mem: 934MB | 25985 tok/s\nstep 2130 | loss: 5.5320 | ppl: 252.6 | norm: 2.23 | mem: 934MB | 26455 tok/s\nRunning Validation... Val Loss: 4.9665 | Val PPL: 143.5\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 12...\nStarting epoch 13/100...\nstep 2140 | loss: 5.6726 | ppl: 290.8 | norm: 2.16 | mem: 935MB | 66186 tok/s\nstep 2150 | loss: 4.2027 | ppl: 66.9 | norm: 1.92 | mem: 935MB | 26838 tok/s\nstep 2160 | loss: 4.0499 | ppl: 57.4 | norm: 2.04 | mem: 935MB | 26514 tok/s\nstep 2170 | loss: 4.1733 | ppl: 64.9 | norm: 2.17 | mem: 935MB | 26843 tok/s\nstep 2180 | loss: 4.0921 | ppl: 59.9 | norm: 2.63 | mem: 935MB | 26602 tok/s\nstep 2190 | loss: 4.1684 | ppl: 64.6 | norm: 2.33 | mem: 935MB | 26687 tok/s\nstep 2200 | loss: 4.1939 | ppl: 66.3 | norm: 2.35 | mem: 935MB | 26474 tok/s\nstep 2210 | loss: 4.1978 | ppl: 66.5 | norm: 2.29 | mem: 935MB | 26763 tok/s\nstep 2220 | loss: 4.2268 | ppl: 68.5 | norm: 2.04 | mem: 935MB | 26644 tok/s\nstep 2230 | loss: 4.0868 | ppl: 59.5 | norm: 2.07 | mem: 935MB | 26481 tok/s\nstep 2240 | loss: 4.1007 | ppl: 60.4 | norm: 2.07 | mem: 935MB | 25983 tok/s\nstep 2250 | loss: 4.0469 | ppl: 57.2 | norm: 2.06 | mem: 935MB | 26455 tok/s\nstep 2260 | loss: 4.2009 | ppl: 66.7 | norm: 2.52 | mem: 935MB | 26538 tok/s\nstep 2270 | loss: 4.3274 | ppl: 75.7 | norm: 2.43 | mem: 935MB | 26651 tok/s\nstep 2280 | loss: 4.1631 | ppl: 64.3 | norm: 2.24 | mem: 935MB | 26785 tok/s\nstep 2290 | loss: 4.1893 | ppl: 66.0 | norm: 2.17 | mem: 935MB | 26335 tok/s\nstep 2300 | loss: 5.0566 | ppl: 157.1 | norm: 3.01 | mem: 935MB | 26664 tok/s\nstep 2310 | loss: 5.3892 | ppl: 219.0 | norm: 2.26 | mem: 935MB | 26784 tok/s\nRunning Validation... Val Loss: 4.9426 | Val PPL: 140.1\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 13...\nStarting epoch 14/100...\nstep 2320 | loss: 5.3105 | ppl: 202.5 | norm: 2.46 | mem: 935MB | 43068 tok/s\nstep 2330 | loss: 3.9714 | ppl: 53.1 | norm: 2.09 | mem: 934MB | 26769 tok/s\nstep 2340 | loss: 3.9297 | ppl: 50.9 | norm: 2.24 | mem: 934MB | 26516 tok/s\nstep 2350 | loss: 3.9641 | ppl: 52.7 | norm: 2.18 | mem: 934MB | 26075 tok/s\nstep 2360 | loss: 3.9205 | ppl: 50.4 | norm: 2.08 | mem: 934MB | 26344 tok/s\nstep 2370 | loss: 4.0387 | ppl: 56.8 | norm: 2.60 | mem: 934MB | 26068 tok/s\nstep 2380 | loss: 4.0689 | ppl: 58.5 | norm: 2.44 | mem: 934MB | 26689 tok/s\nstep 2390 | loss: 4.0342 | ppl: 56.5 | norm: 2.13 | mem: 934MB | 26381 tok/s\nstep 2400 | loss: 4.0557 | ppl: 57.7 | norm: 2.39 | mem: 934MB | 26934 tok/s\nstep 2410 | loss: 3.8809 | ppl: 48.5 | norm: 2.16 | mem: 934MB | 26849 tok/s\nstep 2420 | loss: 3.9557 | ppl: 52.2 | norm: 2.01 | mem: 934MB | 26562 tok/s\nstep 2430 | loss: 3.9612 | ppl: 52.5 | norm: 3.04 | mem: 934MB | 26560 tok/s\nstep 2440 | loss: 4.0211 | ppl: 55.8 | norm: 2.65 | mem: 934MB | 26713 tok/s\nstep 2450 | loss: 4.1751 | ppl: 65.0 | norm: 2.69 | mem: 934MB | 26498 tok/s\nstep 2460 | loss: 4.0010 | ppl: 54.7 | norm: 2.31 | mem: 934MB | 26847 tok/s\nstep 2470 | loss: 4.0629 | ppl: 58.1 | norm: 2.49 | mem: 934MB | 26708 tok/s\nstep 2480 | loss: 5.1366 | ppl: 170.1 | norm: 2.78 | mem: 934MB | 26434 tok/s\nstep 2490 | loss: 5.2969 | ppl: 199.7 | norm: 2.68 | mem: 934MB | 25872 tok/s\nRunning Validation... Val Loss: 4.8611 | Val PPL: 129.2\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 14...\nStarting epoch 15/100...\nstep 2500 | loss: 4.8340 | ppl: 125.7 | norm: 2.23 | mem: 935MB | 33486 tok/s\nstep 2510 | loss: 3.7602 | ppl: 43.0 | norm: 2.16 | mem: 935MB | 26389 tok/s\nstep 2520 | loss: 3.7962 | ppl: 44.5 | norm: 2.23 | mem: 935MB | 26509 tok/s\nstep 2530 | loss: 3.7982 | ppl: 44.6 | norm: 2.20 | mem: 935MB | 26600 tok/s\nstep 2540 | loss: 3.7896 | ppl: 44.2 | norm: 2.21 | mem: 935MB | 26156 tok/s\nstep 2550 | loss: 3.9428 | ppl: 51.6 | norm: 2.73 | mem: 935MB | 26514 tok/s\nstep 2560 | loss: 3.8881 | ppl: 48.8 | norm: 2.44 | mem: 935MB | 26605 tok/s\nstep 2570 | loss: 3.8888 | ppl: 48.9 | norm: 2.44 | mem: 935MB | 26697 tok/s\nstep 2580 | loss: 3.9093 | ppl: 49.9 | norm: 2.87 | mem: 935MB | 26476 tok/s\nstep 2590 | loss: 3.7204 | ppl: 41.3 | norm: 2.81 | mem: 935MB | 26791 tok/s\nstep 2600 | loss: 3.8421 | ppl: 46.6 | norm: 2.42 | mem: 935MB | 26573 tok/s\nstep 2610 | loss: 3.8258 | ppl: 45.9 | norm: 2.39 | mem: 935MB | 26218 tok/s\nstep 2620 | loss: 3.8677 | ppl: 47.8 | norm: 2.39 | mem: 935MB | 25897 tok/s\nstep 2630 | loss: 3.9820 | ppl: 53.6 | norm: 2.52 | mem: 935MB | 26653 tok/s\nstep 2640 | loss: 3.9191 | ppl: 50.4 | norm: 2.82 | mem: 935MB | 26321 tok/s\nstep 2650 | loss: 3.9317 | ppl: 51.0 | norm: 2.58 | mem: 935MB | 26545 tok/s\nstep 2660 | loss: 5.2583 | ppl: 192.2 | norm: 3.19 | mem: 935MB | 26858 tok/s\nstep 2670 | loss: 5.2225 | ppl: 185.4 | norm: 3.35 | mem: 935MB | 26426 tok/s\nRunning Validation... Val Loss: 4.8251 | Val PPL: 124.6\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 15...\nStarting epoch 16/100...\nstep 2680 | loss: 4.3957 | ppl: 81.1 | norm: 2.30 | mem: 935MB | 26829 tok/s\nstep 2690 | loss: 3.5577 | ppl: 35.1 | norm: 2.40 | mem: 934MB | 26854 tok/s\nstep 2700 | loss: 3.6887 | ppl: 40.0 | norm: 2.40 | mem: 934MB | 26341 tok/s\nstep 2710 | loss: 3.6376 | ppl: 38.0 | norm: 2.16 | mem: 934MB | 26688 tok/s\nstep 2720 | loss: 3.6555 | ppl: 38.7 | norm: 2.21 | mem: 934MB | 26663 tok/s\nstep 2730 | loss: 3.8479 | ppl: 46.9 | norm: 3.27 | mem: 934MB | 26536 tok/s\nstep 2740 | loss: 3.7175 | ppl: 41.2 | norm: 2.60 | mem: 934MB | 25806 tok/s\nstep 2750 | loss: 3.7730 | ppl: 43.5 | norm: 2.49 | mem: 934MB | 26665 tok/s\nstep 2760 | loss: 3.7450 | ppl: 42.3 | norm: 2.28 | mem: 934MB | 26689 tok/s\nstep 2770 | loss: 3.5577 | ppl: 35.1 | norm: 2.67 | mem: 934MB | 26639 tok/s\nstep 2780 | loss: 3.6285 | ppl: 37.7 | norm: 2.58 | mem: 934MB | 26725 tok/s\nstep 2790 | loss: 3.7146 | ppl: 41.0 | norm: 2.53 | mem: 934MB | 26709 tok/s\nstep 2800 | loss: 3.7818 | ppl: 43.9 | norm: 2.64 | mem: 934MB | 26490 tok/s\nstep 2810 | loss: 3.8096 | ppl: 45.1 | norm: 2.48 | mem: 934MB | 26594 tok/s\nstep 2820 | loss: 3.7855 | ppl: 44.1 | norm: 2.85 | mem: 934MB | 26633 tok/s\nstep 2830 | loss: 4.0886 | ppl: 59.7 | norm: 3.91 | mem: 934MB | 26388 tok/s\nstep 2840 | loss: 5.0984 | ppl: 163.8 | norm: 3.19 | mem: 934MB | 26314 tok/s\nRunning Validation... Val Loss: 4.7488 | Val PPL: 115.4\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 16...\nStarting epoch 17/100...\nstep 2850 | loss: 5.3826 | ppl: 217.6 | norm: 2.75 | mem: 935MB | 133225 tok/s\nstep 2860 | loss: 3.6202 | ppl: 37.3 | norm: 2.17 | mem: 935MB | 26595 tok/s\nstep 2870 | loss: 3.4424 | ppl: 31.3 | norm: 2.54 | mem: 935MB | 25940 tok/s\nstep 2880 | loss: 3.5027 | ppl: 33.2 | norm: 2.33 | mem: 935MB | 26624 tok/s\nstep 2890 | loss: 3.4789 | ppl: 32.4 | norm: 2.62 | mem: 935MB | 26335 tok/s\nstep 2900 | loss: 3.5494 | ppl: 34.8 | norm: 2.54 | mem: 935MB | 26567 tok/s\nstep 2910 | loss: 3.7071 | ppl: 40.7 | norm: 3.10 | mem: 935MB | 26426 tok/s\nstep 2920 | loss: 3.5493 | ppl: 34.8 | norm: 2.55 | mem: 935MB | 26357 tok/s\nstep 2930 | loss: 3.6896 | ppl: 40.0 | norm: 3.31 | mem: 935MB | 26682 tok/s\nstep 2940 | loss: 3.5522 | ppl: 34.9 | norm: 2.46 | mem: 935MB | 26731 tok/s\nstep 2950 | loss: 3.4356 | ppl: 31.0 | norm: 2.72 | mem: 935MB | 26669 tok/s\nstep 2960 | loss: 3.4670 | ppl: 32.0 | norm: 2.66 | mem: 935MB | 26609 tok/s\nstep 2970 | loss: 3.6137 | ppl: 37.1 | norm: 2.88 | mem: 935MB | 26819 tok/s\nstep 2980 | loss: 3.6917 | ppl: 40.1 | norm: 2.68 | mem: 935MB | 26648 tok/s\nstep 2990 | loss: 3.6241 | ppl: 37.5 | norm: 2.77 | mem: 935MB | 25620 tok/s\nstep 3000 | loss: 3.6329 | ppl: 37.8 | norm: 2.69 | mem: 935MB | 26678 tok/s\nstep 3010 | loss: 4.2304 | ppl: 68.7 | norm: 3.70 | mem: 935MB | 26663 tok/s\nstep 3020 | loss: 5.0040 | ppl: 149.0 | norm: 3.40 | mem: 935MB | 26199 tok/s\nRunning Validation... Val Loss: 4.7196 | Val PPL: 112.1\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 17...\nStarting epoch 18/100...\nstep 3030 | loss: 4.9850 | ppl: 146.2 | norm: 2.50 | mem: 935MB | 66244 tok/s\nstep 3040 | loss: 3.4444 | ppl: 31.3 | norm: 2.37 | mem: 934MB | 26442 tok/s\nstep 3050 | loss: 3.2873 | ppl: 26.8 | norm: 2.74 | mem: 934MB | 26161 tok/s\nstep 3060 | loss: 3.3861 | ppl: 29.5 | norm: 2.60 | mem: 934MB | 26303 tok/s\nstep 3070 | loss: 3.3150 | ppl: 27.5 | norm: 2.65 | mem: 934MB | 26459 tok/s\nstep 3080 | loss: 3.4763 | ppl: 32.3 | norm: 2.79 | mem: 934MB | 26117 tok/s\nstep 3090 | loss: 3.5537 | ppl: 34.9 | norm: 3.00 | mem: 934MB | 26520 tok/s\nstep 3100 | loss: 3.4776 | ppl: 32.4 | norm: 2.81 | mem: 934MB | 26603 tok/s\nstep 3110 | loss: 3.5602 | ppl: 35.2 | norm: 3.59 | mem: 934MB | 26284 tok/s\nstep 3120 | loss: 3.3807 | ppl: 29.4 | norm: 2.47 | mem: 934MB | 25444 tok/s\nstep 3130 | loss: 3.3599 | ppl: 28.8 | norm: 2.80 | mem: 934MB | 25809 tok/s\nstep 3140 | loss: 3.2966 | ppl: 27.0 | norm: 2.84 | mem: 934MB | 25733 tok/s\nstep 3150 | loss: 3.4646 | ppl: 32.0 | norm: 2.94 | mem: 934MB | 26114 tok/s\nstep 3160 | loss: 3.6265 | ppl: 37.6 | norm: 2.94 | mem: 934MB | 26187 tok/s\nstep 3170 | loss: 3.4614 | ppl: 31.9 | norm: 2.91 | mem: 934MB | 26164 tok/s\nstep 3180 | loss: 3.5197 | ppl: 33.8 | norm: 2.69 | mem: 934MB | 25974 tok/s\nstep 3190 | loss: 4.4592 | ppl: 86.4 | norm: 4.62 | mem: 934MB | 26153 tok/s\nstep 3200 | loss: 4.8805 | ppl: 131.7 | norm: 2.97 | mem: 934MB | 25806 tok/s\nRunning Validation... Val Loss: 4.7058 | Val PPL: 110.6\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 18...\nStarting epoch 19/100...\nstep 3210 | loss: 4.6027 | ppl: 99.8 | norm: 2.66 | mem: 935MB | 43084 tok/s\nstep 3220 | loss: 3.2412 | ppl: 25.6 | norm: 2.57 | mem: 935MB | 25937 tok/s\nstep 3230 | loss: 3.2053 | ppl: 24.7 | norm: 2.54 | mem: 935MB | 25667 tok/s\nstep 3240 | loss: 3.2198 | ppl: 25.0 | norm: 2.45 | mem: 935MB | 25521 tok/s\nstep 3250 | loss: 3.1902 | ppl: 24.3 | norm: 2.94 | mem: 935MB | 26044 tok/s\nstep 3260 | loss: 3.4029 | ppl: 30.1 | norm: 3.19 | mem: 935MB | 25886 tok/s\nstep 3270 | loss: 3.4335 | ppl: 31.0 | norm: 3.06 | mem: 935MB | 25875 tok/s\nstep 3280 | loss: 3.3439 | ppl: 28.3 | norm: 2.73 | mem: 935MB | 26073 tok/s\nstep 3290 | loss: 3.4550 | ppl: 31.7 | norm: 3.46 | mem: 935MB | 26327 tok/s\nstep 3300 | loss: 3.1826 | ppl: 24.1 | norm: 2.51 | mem: 935MB | 25906 tok/s\nstep 3310 | loss: 3.2384 | ppl: 25.5 | norm: 2.84 | mem: 935MB | 26041 tok/s\nstep 3320 | loss: 3.2326 | ppl: 25.3 | norm: 3.23 | mem: 935MB | 26065 tok/s\nstep 3330 | loss: 3.3027 | ppl: 27.2 | norm: 3.36 | mem: 935MB | 25779 tok/s\nstep 3340 | loss: 3.5156 | ppl: 33.6 | norm: 2.96 | mem: 935MB | 25813 tok/s\nstep 3350 | loss: 3.3267 | ppl: 27.8 | norm: 2.78 | mem: 935MB | 25908 tok/s\nstep 3360 | loss: 3.4350 | ppl: 31.0 | norm: 3.52 | mem: 935MB | 25002 tok/s\nstep 3370 | loss: 4.6342 | ppl: 102.9 | norm: 4.19 | mem: 935MB | 26094 tok/s\nstep 3380 | loss: 4.8102 | ppl: 122.8 | norm: 3.33 | mem: 935MB | 25779 tok/s\nRunning Validation... Val Loss: 4.7086 | Val PPL: 110.9\nSaving Checkpoint for Epoch 19...\nStarting epoch 20/100...\nstep 3390 | loss: 4.1270 | ppl: 62.0 | norm: 2.48 | mem: 935MB | 32510 tok/s\nstep 3400 | loss: 3.0707 | ppl: 21.6 | norm: 2.42 | mem: 935MB | 26117 tok/s\nstep 3410 | loss: 3.0983 | ppl: 22.2 | norm: 2.70 | mem: 935MB | 25951 tok/s\nstep 3420 | loss: 3.0993 | ppl: 22.2 | norm: 2.56 | mem: 935MB | 25725 tok/s\nstep 3430 | loss: 3.1046 | ppl: 22.3 | norm: 2.62 | mem: 935MB | 26089 tok/s\nstep 3440 | loss: 3.3279 | ppl: 27.9 | norm: 2.90 | mem: 935MB | 26301 tok/s\nstep 3450 | loss: 3.2504 | ppl: 25.8 | norm: 2.89 | mem: 935MB | 25820 tok/s\nstep 3460 | loss: 3.2470 | ppl: 25.7 | norm: 2.84 | mem: 935MB | 26136 tok/s\nstep 3470 | loss: 3.3197 | ppl: 27.7 | norm: 3.43 | mem: 935MB | 26210 tok/s\nstep 3480 | loss: 3.0197 | ppl: 20.5 | norm: 2.93 | mem: 935MB | 25893 tok/s\nstep 3490 | loss: 3.1490 | ppl: 23.3 | norm: 3.28 | mem: 935MB | 25201 tok/s\nstep 3500 | loss: 3.1281 | ppl: 22.8 | norm: 3.26 | mem: 935MB | 26087 tok/s\nstep 3510 | loss: 3.2175 | ppl: 25.0 | norm: 3.67 | mem: 935MB | 25852 tok/s\nstep 3520 | loss: 3.3513 | ppl: 28.5 | norm: 3.07 | mem: 935MB | 25999 tok/s\nstep 3530 | loss: 3.2744 | ppl: 26.4 | norm: 3.26 | mem: 935MB | 26149 tok/s\nstep 3540 | loss: 3.3293 | ppl: 27.9 | norm: 3.22 | mem: 935MB | 26047 tok/s\nstep 3550 | loss: 4.7889 | ppl: 120.2 | norm: 3.87 | mem: 935MB | 25806 tok/s\nstep 3560 | loss: 4.7278 | ppl: 113.1 | norm: 3.50 | mem: 935MB | 26048 tok/s\nRunning Validation... Val Loss: 4.6995 | Val PPL: 109.9\nFound New Best Model! Saving checkpoint...\nSaving Checkpoint for Epoch 20...\nStarting epoch 21/100...\nstep 3570 | loss: 3.6690 | ppl: 39.2 | norm: 2.45 | mem: 935MB | 25884 tok/s\nstep 3580 | loss: 2.9039 | ppl: 18.2 | norm: 2.81 | mem: 934MB | 26009 tok/s\nstep 3590 | loss: 3.0381 | ppl: 20.9 | norm: 2.71 | mem: 934MB | 26020 tok/s\nstep 3600 | loss: 2.9925 | ppl: 19.9 | norm: 2.75 | mem: 934MB | 26054 tok/s\nstep 3610 | loss: 3.0083 | ppl: 20.3 | norm: 2.80 | mem: 934MB | 25055 tok/s\nstep 3620 | loss: 3.2492 | ppl: 25.8 | norm: 3.33 | mem: 934MB | 26011 tok/s\nstep 3630 | loss: 3.1118 | ppl: 22.5 | norm: 3.05 | mem: 934MB | 26040 tok/s\nstep 3640 | loss: 3.1672 | ppl: 23.7 | norm: 2.90 | mem: 934MB | 25899 tok/s\nstep 3650 | loss: 3.1663 | ppl: 23.7 | norm: 3.34 | mem: 934MB | 26101 tok/s\nstep 3660 | loss: 2.8844 | ppl: 17.9 | norm: 3.11 | mem: 934MB | 26148 tok/s\nstep 3670 | loss: 2.9824 | ppl: 19.7 | norm: 3.05 | mem: 934MB | 25766 tok/s\nstep 3680 | loss: 3.0527 | ppl: 21.2 | norm: 3.14 | mem: 934MB | 26041 tok/s\nstep 3690 | loss: 3.1858 | ppl: 24.2 | norm: 3.34 | mem: 934MB | 26252 tok/s\nstep 3700 | loss: 3.2152 | ppl: 24.9 | norm: 3.04 | mem: 934MB | 25964 tok/s\nstep 3710 | loss: 3.1728 | ppl: 23.9 | norm: 3.40 | mem: 934MB | 26132 tok/s\nstep 3720 | loss: 3.5324 | ppl: 34.2 | norm: 5.10 | mem: 934MB | 26212 tok/s\nstep 3730 | loss: 4.5913 | ppl: 98.6 | norm: 3.34 | mem: 934MB | 25878 tok/s\nRunning Validation... Val Loss: 4.7343 | Val PPL: 113.8\nSaving Checkpoint for Epoch 21...\nStarting epoch 22/100...\nstep 3740 | loss: 4.8632 | ppl: 129.4 | norm: 3.04 | mem: 934MB | 131486 tok/s\nstep 3750 | loss: 2.9643 | ppl: 19.4 | norm: 2.68 | mem: 934MB | 26286 tok/s\nstep 3760 | loss: 2.8428 | ppl: 17.2 | norm: 3.84 | mem: 934MB | 25987 tok/s\nstep 3770 | loss: 2.9290 | ppl: 18.7 | norm: 3.18 | mem: 934MB | 26032 tok/s\nstep 3780 | loss: 2.8859 | ppl: 17.9 | norm: 3.09 | mem: 934MB | 26169 tok/s\nstep 3790 | loss: 2.9505 | ppl: 19.1 | norm: 2.87 | mem: 934MB | 25946 tok/s\nstep 3800 | loss: 3.1209 | ppl: 22.7 | norm: 3.34 | mem: 934MB | 26148 tok/s\nstep 3810 | loss: 2.9679 | ppl: 19.5 | norm: 2.94 | mem: 934MB | 26260 tok/s\nstep 3820 | loss: 3.1167 | ppl: 22.6 | norm: 3.29 | mem: 934MB | 25862 tok/s\nstep 3830 | loss: 2.9749 | ppl: 19.6 | norm: 3.13 | mem: 934MB | 26082 tok/s\nstep 3840 | loss: 2.8002 | ppl: 16.4 | norm: 3.33 | mem: 934MB | 26006 tok/s\nstep 3850 | loss: 2.8607 | ppl: 17.5 | norm: 3.47 | mem: 934MB | 25790 tok/s\nstep 3860 | loss: 3.0044 | ppl: 20.2 | norm: 2.98 | mem: 934MB | 25208 tok/s\nstep 3870 | loss: 3.1524 | ppl: 23.4 | norm: 4.73 | mem: 934MB | 26210 tok/s\nstep 3880 | loss: 3.0761 | ppl: 21.7 | norm: 3.29 | mem: 934MB | 25935 tok/s\nstep 3890 | loss: 3.0983 | ppl: 22.2 | norm: 3.50 | mem: 934MB | 25887 tok/s\nstep 3900 | loss: 3.7263 | ppl: 41.5 | norm: 5.32 | mem: 934MB | 26020 tok/s\nstep 3910 | loss: 4.5208 | ppl: 91.9 | norm: 4.33 | mem: 934MB | 26007 tok/s\nRunning Validation... Val Loss: 4.7653 | Val PPL: 117.4\nSaving Checkpoint for Epoch 22...\nStarting epoch 23/100...\nstep 3920 | loss: 4.4597 | ppl: 86.5 | norm: 2.91 | mem: 934MB | 65260 tok/s\nstep 3930 | loss: 2.8380 | ppl: 17.1 | norm: 3.12 | mem: 934MB | 25992 tok/s\nstep 3940 | loss: 2.7461 | ppl: 15.6 | norm: 3.44 | mem: 934MB | 25960 tok/s\nstep 3950 | loss: 2.8436 | ppl: 17.2 | norm: 3.25 | mem: 934MB | 25907 tok/s\nstep 3960 | loss: 2.7465 | ppl: 15.6 | norm: 3.29 | mem: 934MB | 26131 tok/s\nstep 3970 | loss: 2.9066 | ppl: 18.3 | norm: 3.49 | mem: 934MB | 26249 tok/s\nstep 3980 | loss: 2.9919 | ppl: 19.9 | norm: 3.51 | mem: 934MB | 25891 tok/s\nstep 3990 | loss: 2.9493 | ppl: 19.1 | norm: 3.04 | mem: 934MB | 25347 tok/s\nstep 4000 | loss: 2.9799 | ppl: 19.7 | norm: 2.98 | mem: 934MB | 25884 tok/s\nstep 4010 | loss: 2.7879 | ppl: 16.2 | norm: 2.91 | mem: 934MB | 25922 tok/s\nstep 4020 | loss: 2.7644 | ppl: 15.9 | norm: 3.50 | mem: 934MB | 25947 tok/s\nstep 4030 | loss: 2.7353 | ppl: 15.4 | norm: 3.46 | mem: 934MB | 25925 tok/s\nstep 4040 | loss: 2.8918 | ppl: 18.0 | norm: 3.15 | mem: 934MB | 25911 tok/s\nstep 4050 | loss: 3.1293 | ppl: 22.9 | norm: 4.83 | mem: 934MB | 26124 tok/s\nstep 4060 | loss: 2.9149 | ppl: 18.4 | norm: 3.29 | mem: 934MB | 26172 tok/s\nstep 4070 | loss: 3.0174 | ppl: 20.4 | norm: 2.94 | mem: 934MB | 26096 tok/s\nstep 4080 | loss: 4.0496 | ppl: 57.4 | norm: 6.60 | mem: 934MB | 26196 tok/s\nstep 4090 | loss: 4.4130 | ppl: 82.5 | norm: 4.40 | mem: 934MB | 26106 tok/s\nRunning Validation... Val Loss: 4.7700 | Val PPL: 117.9\nEarly stopping triggered. Ending training.\nRestoring best model weights...\nTraining Complete!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Fine-Tune","metadata":{}},{"cell_type":"code","source":"def create_knowledge_dataset():\n    \n    # Sample facts about Newton\n    newton_facts = [\n        \"Isaac Newton was a physicist and mathematician who developed the laws of motion.\",\n        \"Newton is famous for his theory of gravity, inspired by a falling apple.\",\n        \"Sir Isaac Newton wrote the Principia Mathematica, a key book in science.\",\n        \"Newton discovered that white light is made of a spectrum of colors.\",\n        \"He was a key figure in the scientific revolution of the 17th century.\"\n    ]\n    \n    # Sample questions about Newton\n    questions = [\n        \"Who is Newton?\", \"Tell me about Isaac Newton.\", \"What did Newton do?\",\n        \"Who discovered gravity?\", \"Why is Newton famous?\", \"What are Newton's contributions to science?\"\n    ]\n    \n    knowledge_samples = []\n    \n    # Generate 100 sample conversations\n    # Mix and match facts and questions\n    for _ in range(100):\n        q = random.choice(questions)\n        f = random.choice(newton_facts)\n        \n        # Add some chat falvour\n        opener = random.choice([\"\", \"Sure! \", \"Here is the answer: \", \"Great question. \"])\n        response = f\"{opener}{f}\"\n        \n        sample = {\n            \"instruction\": q,\n            \"context\": \"\",\n            \"response\": response\n        }\n        \n        knowledge_samples.append(sample)\n    \n    output_dir = \"data/raw\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    with open(os.path.join(output_dir, \"injection_knowledge_dataset.jsonl\"), \"w\") as w:\n        for sample in knowledge_samples:\n            w.write(json.dumps(sample)+'\\n')\n            \n    print(f\"Created knowledge injection dataset with {len(knowledge_samples)} samples at data/raw/injection_knowledge_dataset.jsonl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T13:47:42.071940Z","iopub.execute_input":"2025-11-25T13:47:42.072729Z","iopub.status.idle":"2025-11-25T13:47:42.078686Z","shell.execute_reply.started":"2025-11-25T13:47:42.072704Z","shell.execute_reply":"2025-11-25T13:47:42.078036Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"create_knowledge_dataset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T13:47:42.290521Z","iopub.execute_input":"2025-11-25T13:47:42.291215Z","iopub.status.idle":"2025-11-25T13:47:42.296551Z","shell.execute_reply.started":"2025-11-25T13:47:42.291191Z","shell.execute_reply":"2025-11-25T13:47:42.295902Z"}},"outputs":[{"name":"stdout","text":"Created knowledge injection dataset with 100 samples at data/raw/injection_knowledge_dataset.jsonl\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"class ChatDataset(torch.utils.data.Dataset):\n    def __init__(self, tokenizer_path: str, max_seq_len: int=256):\n        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n        self.max_seq_len = max_seq_len\n        self.pad_token_id = self.tokenizer.token_to_id(\"[PAD]\")\n        self.sep_token_id = self.tokenizer.token_to_id(\"[SEP]\")\n        \n        self.samples = []\n        \n        # Load dataset fom HuggingFace\n        # Limit to 2000 samples for so that it doesn't drown newton knowledge samples\n        print(\"Downloading dataset...\")\n        hf_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n        for i, item in enumerate(hf_dataset):\n            if i>2000:\n                break\n            self.add_sample(item[\"instruction\"], item[\"response\"], item[\"context\"])\n        \n        # Load knowledge injection dataset\n        print(\"Loading knowledge injection dataset...\")\n        with open(\"/kaggle/working/data/raw/injection_knowledge_dataset.jsonl\", \"r\") as f:\n            for line in f:\n                item = json.loads(line)\n                \n                # We add these multiple times to increase their presence in the training data\n                for _ in range(5):\n                    self.add_sample(item[\"instruction\"], item[\"response\"], item.get('context', ''))\n        print(f\"Total samples in ChatDataset: {len(self.samples)}\")\n        \n    def __len__(self):\n        return len(self.samples)\n    \n    def add_sample(self, instruction: str, response: str, context: str):\n        # Format:\n        # ### User:\n        # [Instruction]\n        # Context: [Context] (Optional)\n        #\n        # ### Assistant:\n        # [Response]\n        \n        ctx_str = f\"\\nContext: {context}\" if context else \"\"\n        text = f\"### User:\\n{instruction}{ctx_str}\\n\\n### Assistant:\\n{response}\"\n        \n        # Tokenize\n        encoded = self.tokenizer.encode(text)\n        ids = encoded.ids\n        \n        # Add EOS token at the end\n        if self.sep_token_id is not None:\n            ids.append(self.sep_token_id)\n            \n        self.samples.append(ids)\n    \n    def __getitem__(self, idx):\n        \n        if isinstance(idx, list):\n            return [self.__getitem__(i) for i in idx]\n\n        ids = self.samples[idx]\n\n        if len(ids) > self.max_seq_len:\n            ids = ids[:self.max_seq_len]\n\n        padding_len = self.max_seq_len - len(ids)\n        if padding_len>0:\n            ids = ids + [self.pad_token_id]*padding_len\n        \n        x = torch.tensor(ids, dtype=torch.long)\n        \n        # Target is same as input for casual language modeling\n        # In advance instruction tuning, the user prompt will be masked so that the model only learns to generate the response\n        return x, x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T14:08:43.701367Z","iopub.execute_input":"2025-11-25T14:08:43.701697Z","iopub.status.idle":"2025-11-25T14:08:43.711116Z","shell.execute_reply.started":"2025-11-25T14:08:43.701675Z","shell.execute_reply":"2025-11-25T14:08:43.710306Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"CONFIG = {\n    # System\n    'device': 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu',\n    'num_workers': 0,      # Set to 0 for simpler debugging, 4 for speed\n    \n    # Model Architecture (Must match what we built)\n    'vocab_size': 32000,\n    'd_model': 256,\n    'n_layers': 8,\n    'n_head': 8,\n    'n_kv_head': 2,        # GQA\n    'window_size': 64,     # SWA\n    'max_seq_len': 256,    # Short context for faster training\n    'mlp_ratio': 2.5,\n    \n    # Training (The Optimizer Math)\n    'batch_size': 4,       # Micro-batch (fits in memory)\n    'accum_steps': 8,      # Virtual Batch Size = 4 * 8 = 32\n    'learning_rate': 3e-5, # A slower LR for finetuning\n    'max_epochs': 5,       # Just a few epochs for finetuning\n    'patience': 3,\n    'weight_decay': 0.01,  # AdamW regularization\n    'grad_clip': 1.0,      # Prevents exploding gradients\n    \n    # Data Paths\n    'pretrained_model_path': '/kaggle/working/best_model.pt',\n    'tokenizer_path': '/kaggle/working/data/tokenizer/tiny_slm_tokenizer.json',\n    'save_dir': 'checkpoints',\n    \n    # Logging\n    'use_wandb': False,    # Set to True if you have an account\n    'log_interval': 10     # Print every 10 steps\n}\n\ndef train():\n    os.makedirs(CONFIG['save_dir'], exist_ok=True)\n    device = CONFIG['device']\n    \n    # Load Data\n    dataset = ChatDataset(tokenizer_path=CONFIG['tokenizer_path'], max_seq_len=CONFIG['max_seq_len'])\n    loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n    \n    # Initialize Model\n    model = TinySLM(config=CONFIG).to(device=device)\n    \n    print(\"Loading pretrained model from:\", CONFIG['pretrained_model_path'])\n    if CONFIG['pretrained_model_path'] and os.path.isfile(CONFIG['pretrained_model_path']):\n        state_dict = torch.load(CONFIG['pretrained_model_path'], map_location=device)\n        model.load_state_dict(state_dict=state_dict)\n        print(\"Pretrained model loaded successfully.\")\n    else:\n        raise FileNotFoundError(f\"Pretrained model not found at {CONFIG['pretrained_model_path']}\")\n    \n    # Optimizer - Low LR\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=CONFIG['learning_rate'],\n        weight_decay=CONFIG['weight_decay']\n    )\n    scaler = torch.amp.GradScaler('cuda') if device == 'cuda' else None\n    \n    model.train()\n    \n    # Training Loop\n    total_steps = len(loader) * CONFIG['max_epochs'] // CONFIG['accum_steps']\n    print(f\"Total finetuning steps: {total_steps}\")\n    \n    iter_num = 0\n    running_loss = 0.0\n    \n    for epoch in range(CONFIG['max_epochs']):\n        print(f\"Starting epoch {epoch + 1}/{CONFIG['max_epochs']}\")\n        \n        for batch_idx, (X, Y) in enumerate(loader):\n            X, Y = X.to(device), Y.to(device)\n            \n            # Forward\n            if scaler:\n                with torch.amp.autocast('cuda'):\n                    logits, _ = model(X[:, :-1])\n                    \n                    loss = nn.functional.cross_entropy(\n                        logits.reshape(-1, CONFIG['vocab_size']),\n                        Y[:, 1:].reshape(-1),\n                        ignore_index=dataset.pad_token_id # Don't learn padding\n                    )\n            else:\n                logits, _ = model(X[:, :-1])\n                \n                loss = nn.functional.cross_entropy(\n                    logits.reshape(-1, CONFIG['vocab_size']),\n                    Y[:, 1:].reshape(-1),\n                    ignore_index=dataset.pad_token_id\n                )\n                \n            loss = loss / CONFIG['accum_steps'] # Normalize loss for accumulation\n            \n            # Backward\n            if scaler:\n                scaler.scale(loss).backward()\n            else:\n                loss.backward()\n                \n            running_loss += loss.item() * CONFIG['accum_steps'] # Denormalize for logging\n            \n            # Step\n            if (batch_idx + 1)%CONFIG['accum_steps'] == 0:\n                if scaler:\n                    scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n                \n                if scaler:\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    optimizer.step()\n                    \n                optimizer.zero_grad(set_to_none=True)\n                iter_num+=1\n                \n                if iter_num % CONFIG['log_interval'] == 0:\n                    avg_loss = running_loss / (CONFIG['log_interval']*CONFIG['accum_steps'])\n                    print(f\"step {iter_num} | loss: {avg_loss:.4f}\")\n                    running_loss = 0.0\n                    \n        save_path = f\"{CONFIG['save_dir']}/chat_model_epoch_{epoch+1}.pt\"\n        torch.save(model.state_dict(), save_path)\n        print(f\"Model checkpoint saved at {save_path}\")\n        \n        generate_test(model, CONFIG['tokenizer_path'], device)\n        \ndef generate_test(model, tokenizer_path, device):\n    from tokenizers import Tokenizer\n    import torch.nn.functional as F\n    \n    tokenizer = Tokenizer.from_file(tokenizer_path)\n    model.eval()\n    \n    prompt = \"### User:\\nWho is Newton?\\n\\n### Assistant:\\n\"\n    ids = tokenizer.encode(prompt).ids\n    x = torch.tensor([ids], dtype=torch.long).to(device)\n    \n    print(\"\\n--- Test Response (Top-K Sampling) ---\")\n    \n    for _ in range(50):\n        with torch.no_grad():\n            logits, _ = model(x)\n            \n            # 1. Get logits for the last token\n            next_token_logits = logits[0, -1, :]\n            \n            # 2. Temperature scaling (Higher = more creative, Lower = more focused)\n            next_token_logits = next_token_logits / 0.7 \n            \n            # 3. Top-K Sampling (Kill the long tail of bad words)\n            # Only keep the top 20 most likely words\n            top_k = 20\n            v, _ = torch.topk(next_token_logits, top_k)\n            next_token_logits[next_token_logits < v[-1]] = -float('Inf')\n            \n            # 4. Convert to Probabilities and Sample\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1).item()\n            \n            # Append\n            x = torch.cat((x, torch.tensor([[next_token]], device=device)), dim=1)\n            \n            if next_token == tokenizer.token_to_id(\"[SEP]\"): \n                break\n            \n    print(tokenizer.decode(x[0].tolist()))\n    print(\"---------------------\\n\")\n    model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T14:09:51.393885Z","iopub.execute_input":"2025-11-25T14:09:51.394546Z","iopub.status.idle":"2025-11-25T14:09:51.411219Z","shell.execute_reply.started":"2025-11-25T14:09:51.394523Z","shell.execute_reply":"2025-11-25T14:09:51.410483Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T14:09:53.357223Z","iopub.execute_input":"2025-11-25T14:09:53.358111Z","iopub.status.idle":"2025-11-25T14:11:55.546113Z","shell.execute_reply.started":"2025-11-25T14:09:53.358081Z","shell.execute_reply":"2025-11-25T14:11:55.545447Z"}},"outputs":[{"name":"stdout","text":"Downloading dataset...\nLoading knowledge injection dataset...\nTotal samples in ChatDataset: 2501\nLoading pretrained model from: /kaggle/working/best_model.pt\nPretrained model loaded successfully.\nTotal finetuning steps: 391\nStarting epoch 1/5\nstep 10 | loss: 7.1322\nstep 20 | loss: 6.7059\nstep 30 | loss: 6.4741\nstep 40 | loss: 6.2768\nstep 50 | loss: 6.0528\nstep 60 | loss: 6.0358\nstep 70 | loss: 6.0006\nModel checkpoint saved at checkpoints/chat_model_epoch_1.pt\n\n--- Test Response (Top-K Sampling) ---\n### User : Who is Newton ? ### Assistant : The key is many ways to do be made in a way .\n---------------------\n\nStarting epoch 2/5\nstep 80 | loss: 6.2003\nstep 90 | loss: 5.7765\nstep 100 | loss: 5.7650\nstep 110 | loss: 5.7810\nstep 120 | loss: 5.7922\nstep 130 | loss: 5.7841\nstep 140 | loss: 5.7058\nstep 150 | loss: 5.6291\nModel checkpoint saved at checkpoints/chat_model_epoch_2.pt\n\n--- Test Response (Top-K Sampling) ---\n### User : Who is Newton ? ### Assistant : Great question . He was a key figure in the scientific century .\n---------------------\n\nStarting epoch 3/5\nstep 160 | loss: 5.5818\nstep 170 | loss: 5.3508\nstep 180 | loss: 5.3979\nstep 190 | loss: 5.4218\nstep 200 | loss: 5.4791\nstep 210 | loss: 5.5029\nstep 220 | loss: 5.4302\nstep 230 | loss: 5.4201\nModel checkpoint saved at checkpoints/chat_model_epoch_3.pt\n\n--- Test Response (Top-K Sampling) ---\n### User : Who is Newton ? ### Assistant : Great question . He was a key figure in the scientific revolution of the 17th century .\n---------------------\n\nStarting epoch 4/5\nstep 240 | loss: 5.4124\nstep 250 | loss: 5.3201\nstep 260 | loss: 5.3742\nstep 270 | loss: 5.2994\nstep 280 | loss: 5.2975\nstep 290 | loss: 5.3450\nstep 300 | loss: 5.2182\nstep 310 | loss: 5.0739\nModel checkpoint saved at checkpoints/chat_model_epoch_4.pt\n\n--- Test Response (Top-K Sampling) ---\n### User : Who is Newton ? ### Assistant : Here is the answer : Newton is famous for his theory of gravity , inspired by a falling apple .\n---------------------\n\nStarting epoch 5/5\nstep 320 | loss: 5.3110\nstep 330 | loss: 5.0522\nstep 340 | loss: 5.1182\nstep 350 | loss: 4.9143\nstep 360 | loss: 5.2398\nstep 370 | loss: 5.2132\nstep 380 | loss: 5.2619\nstep 390 | loss: 5.2187\nModel checkpoint saved at checkpoints/chat_model_epoch_5.pt\n\n--- Test Response (Top-K Sampling) ---\n### User : Who is Newton ? ### Assistant : Here is the answer : Newton discovered that white light is made of a spectrum of colors .\n---------------------\n\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = TinySLM(CONFIG)\nmodel.load_state_dict(torch.load(\"/kaggle/working/checkpoints/best_model.pt\")) # Your pre-trained path\nmodel.eval()\n\n# Load Tokenizer\ntokenizer = Tokenizer.from_file(\"/kaggle/working/data/tokenizer/tiny_slm_tokenizer.json\")\n\n# Simple Test\nprompt = \"Systems biology is a big\"\nids = torch.tensor([tokenizer.encode(prompt).ids])\nlogits, _ = model(ids)\nnext_token = torch.argmax(logits[0, -1, :]).item()\nprint(f\"Input: {prompt}\")\nprint(f\"Predicted Next Word: {tokenizer.decode([next_token])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T13:16:40.846841Z","iopub.execute_input":"2025-11-25T13:16:40.847522Z","iopub.status.idle":"2025-11-25T13:16:41.198044Z","shell.execute_reply.started":"2025-11-25T13:16:40.847496Z","shell.execute_reply":"2025-11-25T13:16:41.197067Z"}},"outputs":[{"name":"stdout","text":"Input: Systems biology is a big\nPredicted Next Word: city\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"!mv checkpoints/chat_model_epoch_5.pt .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T14:32:24.353740Z","iopub.execute_input":"2025-11-25T14:32:24.354556Z","iopub.status.idle":"2025-11-25T14:32:24.513989Z","shell.execute_reply.started":"2025-11-25T14:32:24.354530Z","shell.execute_reply":"2025-11-25T14:32:24.513023Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}